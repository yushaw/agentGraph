# GeneralAgent è¯¦ç»†éœ€æ±‚æ–‡æ¡£ - Part 2: å·¥ä½œåŒºéš”ç¦»ä¸ä¼šè¯ç®¡ç†

## 4. å·¥ä½œåŒºéš”ç¦»éœ€æ±‚

### 4.1 å·¥ä½œåŒºç›®å½•ç»“æ„

**éœ€æ±‚æè¿°**ï¼šæ¯ä¸ªä¼šè¯æ‹¥æœ‰ç‹¬ç«‹çš„å·¥ä½œåŒºç›®å½•ï¼Œçµæ„Ÿæ¥è‡ª OpenAI Code Interpreter å’Œ E2Bã€‚

**ç›®å½•ç»“æ„**ï¼š
```
data/workspace/{session_id}/
â”œâ”€â”€ skills/           # ç¬¦å·é“¾æ¥çš„æŠ€èƒ½ï¼ˆåªè¯»ï¼‰
â”‚   â””â”€â”€ pdf/
â”‚       â”œâ”€â”€ SKILL.md
â”‚       â”œâ”€â”€ forms.md
â”‚       â”œâ”€â”€ reference.md
â”‚       â””â”€â”€ scripts/
â”œâ”€â”€ uploads/          # ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶
â”œâ”€â”€ outputs/          # Agent ç”Ÿæˆçš„è¾“å‡º
â”œâ”€â”€ temp/             # ä¸´æ—¶æ–‡ä»¶
â””â”€â”€ .metadata.json    # ä¼šè¯å…ƒæ•°æ®
```

**å®ç°ä»£ç **ï¼š
```python
# shared/workspace/manager.py:45-75
class WorkspaceManager:
    def __init__(self, base_dir: Path, skill_registry: SkillRegistry):
        self.base_dir = base_dir
        self.skill_registry = skill_registry
        self.workspace_path: Optional[Path] = None

    def create_workspace(self, session_id: str) -> Path:
        """Create isolated workspace for session"""

        workspace = self.base_dir / session_id
        workspace.mkdir(parents=True, exist_ok=True)

        # Create subdirectories
        (workspace / "skills").mkdir(exist_ok=True)
        (workspace / "uploads").mkdir(exist_ok=True)
        (workspace / "outputs").mkdir(exist_ok=True)
        (workspace / "temp").mkdir(exist_ok=True)

        # Save metadata
        metadata = {
            "session_id": session_id,
            "created_at": datetime.now(timezone.utc).isoformat(),
        }
        with open(workspace / ".metadata.json", "w") as f:
            json.dump(metadata, f, indent=2)

        self.workspace_path = workspace
        return workspace
```

### 4.2 è·¯å¾„éš”ç¦»ä¸å®‰å…¨

**éœ€æ±‚æè¿°**ï¼šå·¥å…·åªèƒ½è®¿é—®å·¥ä½œåŒºå†…çš„æ–‡ä»¶ï¼Œé˜²æ­¢è·¯å¾„éå†æ”»å‡»ã€‚

**ä¸¤æ­¥éªŒè¯æœºåˆ¶**ï¼š
```python
# generalAgent/utils/file_processor.py:15-50
def resolve_workspace_path(
    file_path: str,
    workspace_root: Path,
    *,
    must_exist: bool = False,
    allow_write: bool = False,
) -> Path:
    """Resolve and validate workspace-relative path"""

    # Step 1: Resolve logical path (follow symlinks)
    logical_path = (workspace_root / file_path).resolve()

    # Step 2: Check if resolved path is within workspace
    try:
        logical_path.relative_to(workspace_root.resolve())
    except ValueError:
        raise ValueError(f"Path outside workspace: {file_path}")

    # Step 3: Check existence if required
    if must_exist and not logical_path.exists():
        raise FileNotFoundError(f"File not found: {file_path}")

    # Step 4: Check write permissions
    if allow_write:
        allowed_dirs = ["outputs", "temp", "uploads"]
        rel_path = logical_path.relative_to(workspace_root)

        if not any(rel_path.parts[0] == d for d in allowed_dirs):
            raise PermissionError(
                f"Cannot write to {rel_path.parts[0]}/. "
                f"Only {allowed_dirs} are writable."
            )

    return logical_path
```

**åº”ç”¨åˆ°æ–‡ä»¶å·¥å…·**ï¼š
```python
# generalAgent/tools/builtin/file_ops.py:45-60
@tool
def read_file(file_path: str) -> str:
    """Read file from workspace"""

    workspace_root = Path(os.environ.get("AGENT_WORKSPACE_PATH"))

    # Validate path
    abs_path = resolve_workspace_path(
        file_path,
        workspace_root,
        must_exist=True,
        allow_write=False,
    )

    # Read file
    with open(abs_path, "r", encoding="utf-8") as f:
        return f.read()
```

**è®¾è®¡è€ƒé‡**ï¼š
- `resolve()` å¤„ç†ç¬¦å·é“¾æ¥å’Œ `..` è·¯å¾„
- `relative_to()` æ£€æŸ¥æ˜¯å¦åœ¨å·¥ä½œåŒºå†…
- å†™å…¥æƒé™ä»…é™ outputs/, temp/, uploads/
- skills/ ç›®å½•åªè¯»ï¼ˆç¬¦å·é“¾æ¥ï¼‰

### 4.3 æŠ€èƒ½ç¬¦å·é“¾æ¥

**éœ€æ±‚æè¿°**ï¼šå½“ç”¨æˆ· @æåŠæŠ€èƒ½æ—¶ï¼Œå°†æŠ€èƒ½ç›®å½•ç¬¦å·é“¾æ¥åˆ°å·¥ä½œåŒºã€‚

**ç¬¦å·é“¾æ¥é€»è¾‘**ï¼š
```python
# shared/workspace/manager.py:110-145
def load_skill(self, skill_id: str) -> bool:
    """Load skill into workspace by creating symlink"""

    skill = self.skill_registry.get_skill(skill_id)
    if not skill:
        return False

    target_dir = self.workspace_path / "skills" / skill_id

    # Create symlink if not exists
    if not target_dir.exists():
        target_dir.parent.mkdir(parents=True, exist_ok=True)
        target_dir.symlink_to(skill.path, target_is_directory=True)

    # Install dependencies if needed
    requirements = skill.path / "requirements.txt"
    if requirements.exists():
        self._install_skill_dependencies(skill_id, requirements)

    return True
```

**ç¬¦å·é“¾æ¥çš„å¥½å¤„**ï¼š
- ä¸å¤åˆ¶æ–‡ä»¶ï¼ŒèŠ‚çœç©ºé—´
- æŠ€èƒ½æ›´æ–°è‡ªåŠ¨åæ˜ åˆ°æ‰€æœ‰ä¼šè¯
- åªè¯»è®¿é—®ï¼Œé˜²æ­¢è¯¯ä¿®æ”¹

**list_workspace_files å¤„ç†ç¬¦å·é“¾æ¥**ï¼š
```python
# generalAgent/tools/builtin/file_ops.py:214-241
@tool
def list_workspace_files(directory: str = ".") -> str:
    """List files in workspace directory"""

    workspace_root = Path(os.environ.get("AGENT_WORKSPACE_PATH"))

    # Use logical path (don't resolve symlinks)
    logical_path = workspace_root / directory

    # Check within workspace
    try:
        logical_path.relative_to(workspace_root)
    except ValueError:
        return f"Error: Path outside workspace: {directory}"

    # List files
    items = []
    for item in sorted(logical_path.iterdir()):
        rel_path = item.relative_to(workspace_root)

        if item.is_symlink():
            items.append(f"[SKILL] {rel_path}/")
        elif item.is_dir():
            items.append(f"[DIR]  {rel_path}/")
        else:
            size = item.stat().st_size
            items.append(f"[FILE] {rel_path} ({size} bytes)")

    return "\n".join(items)
```

### 4.4 æ–‡ä»¶ä¸Šä¼ å¤„ç†

**éœ€æ±‚æè¿°**ï¼šç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶è‡ªåŠ¨å¤åˆ¶åˆ° `uploads/` ç›®å½•ã€‚

**æ–‡ä»¶ç±»å‹æ£€æµ‹**ï¼š
```python
# generalAgent/utils/file_processor.py:55-80
def detect_file_type(file_path: Path) -> str:
    """Detect file type from extension"""

    ext = file_path.suffix.lower()

    type_map = {
        ".pdf": "pdf",
        ".docx": "document",
        ".xlsx": "spreadsheet",
        ".csv": "csv",
        ".txt": "text",
        ".md": "markdown",
        ".py": "python",
        ".json": "json",
        ".png": "image",
        ".jpg": "image",
        ".jpeg": "image",
    }

    return type_map.get(ext, "unknown")
```

**ä¸Šä¼ å¤„ç†æµç¨‹**ï¼š
```python
# generalAgent/cli.py:180-210
def process_file_upload(self, file_path: str) -> dict:
    """Process uploaded file"""

    src_path = Path(file_path)

    if not src_path.exists():
        return {"success": False, "error": "File not found"}

    # Detect type
    file_type = detect_file_type(src_path)

    # Copy to uploads/
    dest_path = self.workspace_path / "uploads" / src_path.name
    dest_path.parent.mkdir(parents=True, exist_ok=True)

    shutil.copy2(src_path, dest_path)

    # Generate relative path
    rel_path = f"uploads/{src_path.name}"

    return {
        "success": True,
        "path": rel_path,
        "type": file_type,
        "name": src_path.name,
    }
```

**åœ¨æ¶ˆæ¯ä¸­å¼•ç”¨ä¸Šä¼ æ–‡ä»¶**ï¼š
```python
# generalAgent/cli.py:230-245
async def handle_user_message(self, user_input: str, uploaded_files: List[str]):
    """Handle user message with uploaded files"""

    # Process uploads
    file_refs = []
    for file_path in uploaded_files:
        result = self.process_file_upload(file_path)
        if result["success"]:
            file_refs.append(f"- {result['name']} â†’ {result['path']} ({result['type']})")

    # Add file references to message
    if file_refs:
        file_list = "\n".join(file_refs)
        user_input = f"{user_input}\n\nä¸Šä¼ çš„æ–‡ä»¶ï¼š\n{file_list}"

    # ... continue with normal message handling
```

### 4.5 å·¥ä½œåŒºæ¸…ç†

**éœ€æ±‚æè¿°**ï¼šè‡ªåŠ¨æ¸…ç†è¶…è¿‡ 7 å¤©çš„æ—§å·¥ä½œåŒºã€‚

**æ¸…ç†é€»è¾‘**ï¼š
```python
# shared/workspace/manager.py:195-225
def cleanup_old_workspaces(self, days: int = 7):
    """Delete workspaces older than N days"""

    if not self.base_dir.exists():
        return

    cutoff = datetime.now(timezone.utc) - timedelta(days=days)
    deleted_count = 0

    for workspace in self.base_dir.iterdir():
        if not workspace.is_dir():
            continue

        # Read metadata
        metadata_file = workspace / ".metadata.json"
        if not metadata_file.exists():
            continue

        with open(metadata_file) as f:
            metadata = json.load(f)

        # Check age
        created_at = datetime.fromisoformat(metadata["created_at"])
        if created_at < cutoff:
            shutil.rmtree(workspace)
            deleted_count += 1

    return deleted_count
```

**è§¦å‘æ—¶æœº**ï¼š
- ç¨‹åºé€€å‡ºæ—¶è‡ªåŠ¨æ¸…ç†
- ç”¨æˆ·æ‰§è¡Œ `/clean` å‘½ä»¤

**å®ç°**ï¼š
```python
# generalAgent/cli.py:95-105
async def handle_command(self, command: str):
    """Handle slash commands"""

    if command == "/clean":
        count = self.workspace_manager.cleanup_old_workspaces(days=7)
        print(f"âœ“ Cleaned up {count} old workspaces")
        return True

    # ... other commands
```

### 4.6 æ–‡æ¡£è¯»å–ä¸æœç´¢å·¥å…·

**éœ€æ±‚æè¿°**ï¼šæä¾›æ–‡ä»¶æŸ¥æ‰¾ã€è¯»å–å’Œå†…å®¹æœç´¢èƒ½åŠ›ï¼Œæ”¯æŒæ–‡æœ¬æ–‡ä»¶å’Œæ–‡æ¡£æ ¼å¼ï¼ˆPDFã€DOCXã€XLSXã€PPTXï¼‰ã€‚

**æ ¸å¿ƒå·¥å…·**ï¼š

#### find_files - æ–‡ä»¶åæ¨¡å¼åŒ¹é…
```python
# generalAgent/tools/builtin/find_files.py:30-60
@tool
def find_files(
    pattern: Annotated[str, "Glob pattern (e.g., '*.pdf', '**/*.py', '*report*')"],
    path: Annotated[str, "Directory to search (default: workspace root)"] = "."
) -> str:
    """Find files by name pattern (fast, doesn't read file content)."""

    workspace_root = Path(os.environ.get("AGENT_WORKSPACE_PATH"))

    # Resolve search directory
    search_dir = resolve_workspace_path(path, workspace_root, must_exist=True)

    # Find matching files
    matches = list(search_dir.glob(pattern))

    # Filter hidden files and index directories
    matches = [
        f for f in matches
        if not any(part.startswith('.') for part in f.parts)
        and '.indexes' not in f.parts
    ]

    # Sort by modification time (newest first)
    matches.sort(key=lambda p: p.stat().st_mtime, reverse=True)

    return format_results(matches)
```

**ç‰¹ç‚¹**ï¼š
- æ”¯æŒ glob æ¨¡å¼ï¼ˆ`*.pdf`, `**/*.txt`, `*report*`ï¼‰
- è¿‡æ»¤éšè—æ–‡ä»¶å’Œç´¢å¼•ç›®å½•
- æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
- æ˜¾ç¤ºæ–‡ä»¶å¤§å°

#### read_file - æ–‡ä»¶å†…å®¹è¯»å–ï¼ˆå¢å¼ºç‰ˆï¼‰
```python
# generalAgent/tools/builtin/file_ops.py:45-120
@tool
def read_file(file_path: str) -> str:
    """Read file from workspace (text files and documents)"""

    workspace_root = Path(os.environ.get("AGENT_WORKSPACE_PATH"))
    target_path = resolve_workspace_path(file_path, workspace_root, must_exist=True)

    file_ext = target_path.suffix.lower()
    settings = get_settings()

    # Strategy 1: Text files
    if file_ext in TEXT_EXTENSIONS:
        file_size = target_path.stat().st_size

        if file_size < settings.documents.text_file_max_size:
            # Read full content
            with open(target_path, "r", encoding="utf-8") as f:
                return f.read()
        else:
            # Return preview with search hint
            with open(target_path, "r", encoding="utf-8") as f:
                preview = f.read(settings.documents.text_preview_chars)
            return f"{preview}\n\nğŸ’¡ æç¤ºï¼šæ–‡ä»¶è¾ƒå¤§ï¼Œä½¿ç”¨ search_file æœç´¢ç‰¹å®šå†…å®¹"

    # Strategy 2: Document files (PDF, DOCX, XLSX, PPTX)
    if file_ext in DOCUMENT_EXTENSIONS:
        doc_info = get_document_info(target_path)

        if doc_info["pages"] <= 10:
            # Small document: read full content
            return extract_full_document(target_path)
        else:
            # Large document: return preview
            preview = extract_preview(
                target_path,
                max_pages=settings.documents.pdf_preview_pages,
                max_chars=settings.documents.pdf_preview_chars
            )
            return f"{preview}\n\nğŸ’¡ æç¤ºï¼šæ–‡æ¡£è¾ƒå¤§ï¼Œä½¿ç”¨ search_file æœç´¢ç‰¹å®šå†…å®¹"
```

**æ–‡æ¡£å¤„ç†èƒ½åŠ›**ï¼š
- PDFï¼šä½¿ç”¨ pdfplumber æå–æ–‡æœ¬å’Œè¡¨æ ¼
- DOCXï¼šä½¿ç”¨ python-docx æå–æ®µè½å’Œè¡¨æ ¼
- XLSXï¼šä½¿ç”¨ openpyxl è¯»å–å·¥ä½œè¡¨
- PPTXï¼šä½¿ç”¨ python-pptx æå–å¹»ç¯ç‰‡æ–‡æœ¬

**é•¿åº¦é™åˆ¶ç­–ç•¥**ï¼š
- æ–‡æœ¬æ–‡ä»¶ï¼š< 100KB å…¨é‡è¯»å–ï¼Œå¦åˆ™é¢„è§ˆå‰ 50KB
- PDF/DOCXï¼šâ‰¤ 10 é¡µå…¨é‡ï¼Œå¦åˆ™é¢„è§ˆå‰ 10 é¡µ
- XLSXï¼šâ‰¤ 3 sheets å…¨é‡ï¼Œå¦åˆ™é¢„è§ˆå‰ 3 sheets
- PPTXï¼šâ‰¤ 15 slides å…¨é‡ï¼Œå¦åˆ™é¢„è§ˆå‰ 15 slides

#### search_file - å†…å®¹æœç´¢
```python
# generalAgent/tools/builtin/search_file.py:45-120
@tool
def search_file(
    path: Annotated[str, "File path relative to workspace"],
    query: Annotated[str, "Search keywords or phrase"],
    max_results: Annotated[int, "Maximum results to return"] = 5
) -> str:
    """Search for content in a file (supports text files and documents)."""

    workspace_root = Path(os.environ.get("AGENT_WORKSPACE_PATH"))
    target_path = resolve_workspace_path(path, workspace_root, must_exist=True)

    file_ext = target_path.suffix.lower()

    # Strategy 1: Text files - Real-time scanning
    if file_ext in TEXT_EXTENSIONS:
        return _search_text_file(target_path, query, max_results)

    # Strategy 2: Document files - Index-based search
    if file_ext in DOCUMENT_EXTENSIONS:
        return _search_document_file(target_path, query, max_results)
```

**åŒç­–ç•¥æœç´¢**ï¼š

1. **æ–‡æœ¬æ–‡ä»¶**ï¼šå®æ—¶é€è¡Œæ‰«æ
   - ä¸åŒºåˆ†å¤§å°å†™
   - æ˜¾ç¤ºåŒ¹é…è¡ŒåŠå‰åå„ 1 è¡Œä¸Šä¸‹æ–‡
   - é«˜äº®åŒ¹é…æ–‡æœ¬

2. **æ–‡æ¡£æ–‡ä»¶**ï¼šåŸºäºç´¢å¼•çš„æœç´¢
   - é¦–æ¬¡æœç´¢è‡ªåŠ¨åˆ›å»ºç´¢å¼•ï¼ˆå­˜å‚¨åœ¨ `data/indexes/`ï¼‰
   - åç»­æœç´¢ç§’çº§å“åº”ï¼ˆ0.01s vs 0.04sï¼‰
   - å¤šç­–ç•¥è¯„åˆ†ç³»ç»Ÿï¼š
     - çŸ­è¯­åŒ¹é…ï¼š+10 åˆ†
     - ä¸‰å…ƒç»„åŒ¹é…ï¼š+5 åˆ†
     - äºŒå…ƒç»„åŒ¹é…ï¼š+3 åˆ†
     - å…³é”®è¯ç²¾ç¡®ï¼š+2 åˆ†
     - å…³é”®è¯æ¨¡ç³Šï¼š+1 åˆ†
     - è¦†ç›–ç‡å¥–åŠ±ï¼š+0-2 åˆ†

**ç´¢å¼•ç®¡ç†**ï¼š
```python
# generalAgent/utils/text_indexer.py:150-220
def create_index(file_path: Path) -> Path:
    """åˆ›å»ºæ–‡æ¡£æœç´¢ç´¢å¼•"""

    # Compute MD5 hash
    file_hash = compute_file_hash(file_path)

    # Check if index exists
    index_path = get_index_path(file_hash)
    if index_path.exists():
        # Update metadata only
        return index_path

    # Clean up old indexes for same file path (orphan cleanup)
    cleanup_old_indexes_for_file(file_path, keep_hash=file_hash)

    # Extract and chunk document
    chunks = chunk_document(file_path)

    # Build index
    index_data = {
        "file_path": str(file_path),
        "file_hash": file_hash,
        "created_at": datetime.now(timezone.utc).isoformat(),
        "chunks": [
            {
                "chunk_id": i,
                "page_num": chunk["page_num"],
                "text": chunk["text"],
                "keywords": extract_keywords(chunk["text"]),
                "bigrams": extract_ngrams(chunk["text"], n=2),
                "trigrams": extract_ngrams(chunk["text"], n=3)
            }
            for i, chunk in enumerate(chunks)
        ]
    }

    # Save index
    index_path.parent.mkdir(parents=True, exist_ok=True)
    with open(index_path, "w", encoding="utf-8") as f:
        json.dump(index_data, f, ensure_ascii=False, indent=2)

    return index_path
```

**ç´¢å¼•å­˜å‚¨ç­–ç•¥**ï¼š
- **å…¨å±€å­˜å‚¨**ï¼š`data/indexes/{hash[:2]}/{hash}.index.json`
- **ä¸¤çº§ç›®å½•ç»“æ„**ï¼šä½¿ç”¨ hash å‰ä¸¤ä½ä½œä¸ºå­ç›®å½•ï¼ˆ256 ä¸ªå­ç›®å½•ï¼Œé¿å…å•ç›®å½•è¿‡å¤šæ–‡ä»¶ï¼‰
- **MD5 å»é‡**ï¼šç›¸åŒå†…å®¹åªåˆ›å»ºä¸€æ¬¡ç´¢å¼•ï¼ˆè·¨ä¼šè¯å¤ç”¨ï¼‰
- **å­¤å„¿ç´¢å¼•æ¸…ç†**ï¼šä¸Šä¼ åŒåæ–‡ä»¶æ—¶è‡ªåŠ¨åˆ é™¤æ—§ç´¢å¼•
- **è¿‡æœŸæ£€æµ‹**ï¼š24 å°æ—¶æœªè®¿é—®çš„ç´¢å¼•æ ‡è®°ä¸º stale

**å­¤å„¿ç´¢å¼•æ¸…ç†æœºåˆ¶**ï¼š
```python
# generalAgent/utils/text_indexer.py:100-145
def cleanup_old_indexes_for_file(file_path: Path, keep_hash: str):
    """æ¸…ç†æŒ‡å®šæ–‡ä»¶è·¯å¾„çš„æ—§ç´¢å¼•ï¼ˆå¤„ç†åŒåæ–‡ä»¶è¦†ç›–åœºæ™¯ï¼‰

    åœºæ™¯ï¼šç”¨æˆ·åœ¨åŒä¸€ session ä¸Šä¼ åŒåæ–‡ä»¶ä½†å†…å®¹ä¸åŒï¼ˆMD5 ä¸åŒï¼‰
    - æ—§ç´¢å¼•æˆä¸ºå­¤å„¿ï¼ˆfile_path åŒ¹é…ä½† hash ä¸åŒï¼‰
    - æ­¤å‡½æ•°åœ¨åˆ›å»ºæ–°ç´¢å¼•å‰è‡ªåŠ¨æ¸…ç†æ—§ç´¢å¼•
    """

    if not INDEXES_DIR.exists():
        return 0

    deleted_count = 0

    # Scan all index files
    for index_file in INDEXES_DIR.rglob("*.index.json"):
        try:
            with open(index_file, "r", encoding="utf-8") as f:
                index_data = json.load(f)

            # Check if index is for same file path but different hash
            if (index_data.get("file_path") == str(file_path)
                and index_data.get("file_hash") != keep_hash):

                index_file.unlink()
                deleted_count += 1
                LOGGER.info(f"Deleted orphan index: {index_file.name} (replaced by {keep_hash[:8]})")

        except Exception as e:
            LOGGER.debug(f"Error checking index {index_file}: {e}")
            continue

    return deleted_count
```

**é…ç½®**ï¼š
```python
# generalAgent/config/settings.py:115-135
class DocumentSettings(BaseModel):
    """Document reading and indexing settings"""

    # Text file limits
    text_file_max_size: int = 100_000        # 100KB
    text_preview_chars: int = 50_000         # 50KB preview

    # Document preview limits
    pdf_preview_pages: int = 10
    pdf_preview_chars: int = 30_000
    docx_preview_pages: int = 10
    docx_preview_chars: int = 30_000
    xlsx_preview_sheets: int = 3
    xlsx_preview_chars: int = 20_000
    pptx_preview_slides: int = 15
    pptx_preview_chars: int = 25_000

    # Search settings
    search_max_results_default: int = 5
    index_stale_threshold_hours: int = 24
```

**è®¾è®¡è€ƒé‡**ï¼š
- **Unix å“²å­¦**ï¼šä¸‰ä¸ªå·¥å…·å„å¸å…¶èŒï¼ˆfind/read/searchï¼‰ï¼Œé¿å…åŠŸèƒ½æ··æ‚
- **è‡ªåŠ¨ç´¢å¼•**ï¼šé¦–æ¬¡æœç´¢æ—¶è‡ªåŠ¨åˆ›å»ºç´¢å¼•ï¼Œç”¨æˆ·æ— æ„ŸçŸ¥
- **å…¨å±€å»é‡**ï¼šç›¸åŒæ–‡ä»¶è·¨ä¼šè¯å…±äº«ç´¢å¼•ï¼ŒèŠ‚çœå­˜å‚¨å’Œè®¡ç®—
- **å­¤å„¿æ¸…ç†**ï¼šè‡ªåŠ¨å¤„ç†åŒåæ–‡ä»¶è¦†ç›–åœºæ™¯ï¼Œä¿æŒç´¢å¼•ç›®å½•æ•´æ´
- **é•¿åº¦ä¿æŠ¤**ï¼šé¢„è§ˆæœºåˆ¶é˜²æ­¢ä¸Šä¸‹æ–‡æº¢å‡ºï¼Œå¼•å¯¼ç”¨æˆ·ä½¿ç”¨æœç´¢å·¥å…·

---

## 5. ä¼šè¯ç®¡ç†éœ€æ±‚

### 5.1 ä¼šè¯æŒä¹…åŒ–ï¼ˆSQLiteï¼‰

**éœ€æ±‚æè¿°**ï¼šä½¿ç”¨ SQLite æ•°æ®åº“æŒä¹…åŒ–ä¼šè¯çŠ¶æ€ï¼Œæ”¯æŒè·¨æ¬¡è¿è¡Œæ¢å¤å¯¹è¯ã€‚

**æ•°æ®åº“ç»“æ„**ï¼š
```python
# shared/session/store.py:25-50
CREATE_SESSIONS_TABLE = """
CREATE TABLE IF NOT EXISTS sessions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    thread_id TEXT UNIQUE NOT NULL,
    user_id TEXT,
    created_at TEXT NOT NULL,
    updated_at TEXT NOT NULL,
    metadata TEXT
)
"""

CREATE_CHECKPOINTS_TABLE = """
CREATE TABLE IF NOT EXISTS checkpoints (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    thread_id TEXT NOT NULL,
    checkpoint_id TEXT NOT NULL,
    parent_checkpoint_id TEXT,
    checkpoint_data BLOB NOT NULL,
    created_at TEXT NOT NULL,
    FOREIGN KEY (thread_id) REFERENCES sessions (thread_id)
)
"""
```

**SessionStore æ¥å£**ï¼š
```python
# shared/session/store.py:60-125
class SessionStore:
    def __init__(self, db_path: Path):
        self.db_path = db_path
        self._init_db()

    def create_session(self, thread_id: str, user_id: str = None) -> dict:
        """Create new session record"""

    def get_session(self, thread_id: str) -> Optional[dict]:
        """Retrieve session by thread_id"""

    def list_sessions(self, user_id: str = None, limit: int = 20) -> List[dict]:
        """List recent sessions"""

    def delete_session(self, thread_id: str):
        """Delete session and all checkpoints"""

    def save_checkpoint(self, thread_id: str, checkpoint: dict):
        """Save conversation checkpoint"""

    def load_checkpoint(self, thread_id: str) -> Optional[dict]:
        """Load latest checkpoint"""
```

**ä¸ LangGraph Checkpointer é›†æˆ**ï¼š
```python
# generalAgent/persistence/checkpointer.py:15-40
def build_checkpointer(db_path: str) -> Optional[SqliteSaver]:
    """Build SQLite checkpointer for LangGraph"""

    if not db_path:
        return None

    db_file = Path(db_path)
    db_file.parent.mkdir(parents=True, exist_ok=True)

    # Connect to SQLite
    conn = sqlite3.connect(str(db_file), check_same_thread=False)

    # Create LangGraph checkpointer
    checkpointer = SqliteSaver(conn)

    return checkpointer
```

**åº”ç”¨åˆ°å›¾**ï¼š
```python
# generalAgent/runtime/app.py:125-130
checkpointer = build_checkpointer(settings.observability.session_db_path)
if checkpointer:
    LOGGER.info("Session persistence enabled (SQLite)")

app = graph.build_state_graph(
    ...,
    checkpointer=checkpointer,
)
```

### 5.2 ä¼šè¯ç”Ÿå‘½å‘¨æœŸ

**éœ€æ±‚æè¿°**ï¼šç®¡ç†ä¼šè¯çš„åˆ›å»ºã€åŠ è½½ã€é‡ç½®ã€ä¿å­˜æµç¨‹ã€‚

**SessionManager å®ç°**ï¼š
```python
# shared/session/manager.py:25-120
class SessionManager:
    def __init__(
        self,
        session_store: SessionStore,
        workspace_manager: WorkspaceManager,
    ):
        self.session_store = session_store
        self.workspace_manager = workspace_manager
        self.current_session_id: Optional[str] = None

    def create_session(self, user_id: str = None) -> str:
        """Create new session with workspace"""

        thread_id = self._generate_thread_id()

        # Create session record
        self.session_store.create_session(thread_id, user_id)

        # Create workspace
        self.workspace_manager.create_workspace(thread_id)

        self.current_session_id = thread_id
        return thread_id

    def load_session(self, thread_id_prefix: str) -> bool:
        """Load existing session by ID prefix"""

        # Find matching session
        sessions = self.session_store.list_sessions()
        matches = [s for s in sessions if s["thread_id"].startswith(thread_id_prefix)]

        if not matches:
            return False

        session = matches[0]
        thread_id = session["thread_id"]

        # Load workspace
        workspace = self.workspace_manager.base_dir / thread_id
        if not workspace.exists():
            return False

        self.workspace_manager.workspace_path = workspace
        self.current_session_id = thread_id
        return True

    def reset_session(self):
        """Reset current session (clear state but keep workspace)"""

        if not self.current_session_id:
            return

        # Delete checkpoints (keep session record)
        self.session_store.delete_checkpoints(self.current_session_id)

        # Keep workspace but clear temp/
        temp_dir = self.workspace_manager.workspace_path / "temp"
        if temp_dir.exists():
            shutil.rmtree(temp_dir)
            temp_dir.mkdir()
```

**CLI å‘½ä»¤é›†æˆ**ï¼š
```python
# generalAgent/cli.py:50-90
async def handle_command(self, command: str) -> bool:
    """Handle slash commands"""

    if command == "/reset":
        self.session_manager.reset_session()
        print("âœ“ Session reset")
        return True

    if command == "/sessions":
        sessions = self.session_manager.list_sessions()
        for s in sessions:
            print(f"  {s['thread_id'][:8]} - {s['updated_at']}")
        return True

    if command.startswith("/load "):
        prefix = command[6:].strip()
        success = self.session_manager.load_session(prefix)
        if success:
            print(f"âœ“ Loaded session: {self.session_manager.current_session_id}")
        else:
            print(f"âœ— Session not found: {prefix}")
        return True

    if command == "/current":
        if self.session_manager.current_session_id:
            print(f"Current session: {self.session_manager.current_session_id}")
        else:
            print("No active session")
        return True

    return False
```

### 5.3 ä¼šè¯è‡ªåŠ¨ä¿å­˜

**éœ€æ±‚æè¿°**ï¼šæ¯è½®å¯¹è¯ç»“æŸåè‡ªåŠ¨ä¿å­˜ä¼šè¯çŠ¶æ€ã€‚

**å®ç°æ–¹å¼**ï¼š
```python
# generalAgent/cli.py:250-270
async def handle_user_message(self, user_input: str):
    """Handle user message"""

    # ... create user message ...

    # Stream graph execution
    async for chunk in self.app.astream(...):
        # ... process chunks ...
        pass

    # Auto-save session after turn
    if self.session_manager.current_session_id:
        await self._save_session()

async def _save_session(self):
    """Save current session state"""

    # LangGraph checkpointer automatically saves state
    # We only need to update session metadata

    self.session_store.update_session(
        self.session_manager.current_session_id,
        metadata={"last_message": "...", "turn_count": 10}
    )
```

**è®¾è®¡è€ƒé‡**ï¼š
- LangGraph Checkpointer è‡ªåŠ¨ä¿å­˜å›¾çŠ¶æ€
- SessionStore ä¿å­˜é¢å¤–å…ƒæ•°æ®
- æ¯è½®å¯¹è¯åè‡ªåŠ¨è§¦å‘

---

## 6. æ¨¡å‹è·¯ç”±éœ€æ±‚

### 6.1 å¤šæ¨¡å‹æ’æ§½ç³»ç»Ÿ

**éœ€æ±‚æè¿°**ï¼šæ”¯æŒ 5 ç§æ¨¡å‹æ’æ§½ï¼Œæ ¹æ®ä»»åŠ¡ç±»å‹è·¯ç”±åˆ°ä¸åŒæ¨¡å‹ã€‚

**æ¨¡å‹æ’æ§½å®šä¹‰**ï¼š
```python
# generalAgent/config/settings.py:45-75
class ModelSlots(BaseModel):
    base: Optional[ModelConfig] = None       # åŸºç¡€å¯¹è¯
    reasoning: Optional[ModelConfig] = None  # æ·±åº¦æ¨ç†
    vision: Optional[ModelConfig] = None     # å›¾æ–‡ç†è§£
    code: Optional[ModelConfig] = None       # ä»£ç ç”Ÿæˆ
    chat: Optional[ModelConfig] = None       # èŠå¤©å¯¹è¯
```

**ModelConfig å®šä¹‰**ï¼š
```python
class ModelConfig(BaseModel):
    api_key: str
    base_url: str = "https://api.openai.com/v1"
    id: str = "gpt-4"
    temperature: float = 0.7
    max_tokens: Optional[int] = None
```

**ç¯å¢ƒå˜é‡æ˜ å°„**ï¼š
```python
# generalAgent/runtime/model_resolver.py:15-50
def resolve_model_configs(settings: Settings) -> Dict[str, dict]:
    """Resolve model configs from environment variables"""

    configs = {}

    # Map provider aliases to canonical names
    aliases = {
        "MODEL_BASIC_": "base",
        "MODEL_REASONING_": "reasoning",
        "MODEL_MULTIMODAL_": "vision",
        "MODEL_CODE_": "code",
        "MODEL_CHAT_": "chat",
    }

    for prefix, slot in aliases.items():
        api_key = os.getenv(f"{prefix}API_KEY")
        if api_key:
            configs[slot] = {
                "api_key": api_key,
                "base_url": os.getenv(f"{prefix}BASE_URL"),
                "id": os.getenv(f"{prefix}ID"),
            }

    return configs
```

### 6.2 æ¨¡å‹æ³¨å†Œè¡¨

**éœ€æ±‚æè¿°**ï¼šç»Ÿä¸€ç®¡ç†æ‰€æœ‰å·²é…ç½®çš„æ¨¡å‹å®ä¾‹ã€‚

**ModelRegistry å®ç°**ï¼š
```python
# generalAgent/models/registry.py:20-70
class ModelRegistry:
    def __init__(self):
        self._models: Dict[str, BaseChatModel] = {}

    def register(self, slot: str, model: BaseChatModel):
        """Register model for a slot"""
        self._models[slot] = model

    def get(self, slot: str) -> Optional[BaseChatModel]:
        """Get model by slot name"""
        return self._models.get(slot)

    def list_slots(self) -> List[str]:
        """List all registered slots"""
        return list(self._models.keys())

def build_default_registry(model_ids: Dict[str, str]) -> ModelRegistry:
    """Build registry from model IDs"""

    registry = ModelRegistry()

    for slot, model_id in model_ids.items():
        # Get config from environment
        config = resolve_model_config(slot)

        # Create ChatOpenAI instance (works with OpenAI-compatible APIs)
        model = ChatOpenAI(
            api_key=config["api_key"],
            base_url=config["base_url"],
            model=model_id,
            temperature=config.get("temperature", 0.7),
        )

        registry.register(slot, model)

    return registry
```

### 6.3 åŠ¨æ€æ¨¡å‹è§£æ

**éœ€æ±‚æè¿°**ï¼šæ ¹æ®ä»»åŠ¡ç‰¹å¾å’Œç”¨æˆ·åå¥½åŠ¨æ€é€‰æ‹©æ¨¡å‹ã€‚

**ModelResolver æ¥å£**ï¼š
```python
# generalAgent/agents/model_resolver.py:10-30
class ModelResolver:
    def resolve(self, state: AppState, node_name: str) -> str:
        """Resolve model slot for current node"""
        raise NotImplementedError
```

**é»˜è®¤å®ç°**ï¼š
```python
# generalAgent/runtime/model_resolver.py:55-95
class DefaultModelResolver(ModelResolver):
    def __init__(self, model_configs: Dict[str, dict]):
        self.configs = model_configs

    def resolve(self, state: AppState, node_name: str) -> str:
        """Resolve model based on context"""

        # Check user preference
        if state.get("model_pref"):
            return state["model_pref"]

        # Check images (need vision model)
        if state.get("images"):
            if "vision" in self.configs:
                return "vision"

        # Node-specific routing
        if node_name == "agent":
            # Use code model if working with code files
            if self._has_code_context(state):
                return "code"

            # Use reasoning model for complex tasks
            if self._is_complex_task(state):
                return "reasoning"

            # Default to base model
            return "base"

        elif node_name == "finalize":
            # Use chat model for final response
            return "chat" if "chat" in self.configs else "base"

        return "base"
```

**åº”ç”¨åˆ°èŠ‚ç‚¹**ï¼š
```python
# generalAgent/graph/nodes/planner.py:285-295
def planner_node(state: AppState):
    """Agent node with dynamic model selection"""

    # Resolve model
    model_slot = model_resolver.resolve(state, "agent")
    model = model_registry.get(model_slot)

    # Invoke model
    result = model.invoke(messages, tools=visible_tools)

    return {"messages": [result], "loops": state["loops"] + 1}
```

**è®¾è®¡è€ƒé‡**ï¼š
- å¯æ‰©å±•ï¼ˆè‡ªå®šä¹‰ ModelResolverï¼‰
- ä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼ˆæ£€æµ‹å›¾ç‰‡ã€ä»£ç ã€å¤æ‚åº¦ï¼‰
- ç”¨æˆ·å¯è¦†ç›–ï¼ˆmodel_prefï¼‰
