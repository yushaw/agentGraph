"""
Token è¿½è¸ªå’ŒçŠ¶æ€è¯„ä¼°å™¨

è´Ÿè´£ï¼š
1. ä» API å“åº”æå–ç²¾ç¡® token ä½¿ç”¨é‡
2. è®¡ç®—ç´¯ç§¯ä½¿ç”¨é‡å’Œä¸Šä¸‹æ–‡çŠ¶æ€
3. åˆ¤æ–­å“åº”çº§åˆ«ï¼ˆæ­£å¸¸/æç¤º/è­¦å‘Š/å¼ºåˆ¶ï¼‰
4. åŠ¨æ€å†³å®šå‹ç¼©ç­–ç•¥
"""

from dataclasses import dataclass
from typing import Optional, Literal, Dict
from langchain_core.messages import AIMessage
import logging

logger = logging.getLogger(__name__)


@dataclass
class TokenUsage:
    """å•æ¬¡ API è°ƒç”¨çš„ Token ä½¿ç”¨æƒ…å†µ"""
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int
    model_name: str


@dataclass
class ContextStatus:
    """ä¸Šä¸‹æ–‡çŠ¶æ€è¯„ä¼°ç»“æœ"""
    # åŸºç¡€ä¿¡æ¯
    cumulative_prompt_tokens: int
    context_window: int
    usage_ratio: float  # 0.0 to 1.0

    # å“åº”çº§åˆ«
    level: Literal["normal", "info", "warning", "critical"]

    # å‹ç¼©å»ºè®®
    needs_compression: bool

    # ç”¨æˆ·æç¤ºæ¶ˆæ¯
    message: Optional[str]


# æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£é…ç½®ï¼ˆæ”¯æŒåŠ¨æ€æ‰©å±•ï¼‰
MODEL_CONTEXT_WINDOWS: Dict[str, int] = {
    # DeepSeek
    "deepseek-chat": 128_000,
    "deepseek-reasoner": 128_000,

    # Kimi (Moonshot)
    "moonshot-v1-8k": 8_000,
    "moonshot-v1-32k": 32_000,
    "moonshot-v1-128k": 128_000,
    "kimi-k2-0905-preview": 200_000,

    # GLM
    "glm-4": 128_000,
    "glm-4-plus": 128_000,
    "glm-4.5v": 128_000,

    # OpenAI
    "gpt-4": 8_192,
    "gpt-4-32k": 32_768,
    "gpt-4-turbo": 128_000,
    "gpt-4o": 128_000,
    "gpt-4o-mini": 128_000,

    # Claude
    "claude-3-opus": 200_000,
    "claude-3-sonnet": 200_000,
    "claude-3-haiku": 200_000,
    "claude-3-5-sonnet": 200_000,
    "claude-3.5-sonnet": 200_000,

    # é»˜è®¤å€¼
    "default": 128_000
}


class TokenTracker:
    """Token è¿½è¸ªå’ŒçŠ¶æ€è¯„ä¼°å™¨"""

    def __init__(self, settings):
        self.settings = settings
        self.context_settings = settings.context

    def extract_token_usage(self, response: AIMessage) -> Optional[TokenUsage]:
        """
        ä» API å“åº”æå– token ä½¿ç”¨é‡

        Args:
            response: LLM è¿”å›çš„ AIMessage

        Returns:
            TokenUsage å¯¹è±¡ï¼Œå¦‚æœæ— æ³•æå–åˆ™è¿”å› None
        """
        try:
            metadata = response.response_metadata
            usage = metadata.get("token_usage") or metadata.get("usage")

            if not usage:
                logger.warning("No token usage found in response metadata")
                return None

            return TokenUsage(
                prompt_tokens=usage.get("prompt_tokens", 0),
                completion_tokens=usage.get("completion_tokens", 0),
                total_tokens=usage.get("total_tokens", 0),
                model_name=metadata.get("model_name", "unknown")
            )
        except Exception as e:
            logger.error(f"Failed to extract token usage: {e}")
            return None

    def get_context_window(self, model_id: str) -> int:
        """
        è·å–æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£å¤§å°

        æ”¯æŒç²¾ç¡®åŒ¹é…å’Œå‰ç¼€åŒ¹é…ï¼ˆå¦‚ "gpt-4-0125-preview" åŒ¹é… "gpt-4"ï¼‰
        """
        # ç²¾ç¡®åŒ¹é…
        if model_id in MODEL_CONTEXT_WINDOWS:
            return MODEL_CONTEXT_WINDOWS[model_id]

        # å‰ç¼€åŒ¹é…ï¼ˆå¦‚ "deepseek-chat-v2" â†’ "deepseek-chat"ï¼‰
        for key, window in MODEL_CONTEXT_WINDOWS.items():
            if model_id.startswith(key):
                return window

        # é»˜è®¤å€¼
        logger.warning(
            f"Unknown model '{model_id}', using default context window "
            f"{MODEL_CONTEXT_WINDOWS['default']}"
        )
        return MODEL_CONTEXT_WINDOWS["default"]

    def check_status(
        self,
        cumulative_prompt_tokens: int,
        model_id: str
    ) -> ContextStatus:
        """
        æ£€æŸ¥å½“å‰ä¸Šä¸‹æ–‡çŠ¶æ€å¹¶ç»™å‡ºå“åº”å»ºè®®

        å“åº”çº§åˆ«ï¼š
        - normal (0-75%): æ­£å¸¸è¿è¡Œï¼Œæ— éœ€ä»»ä½•æ“ä½œ
        - info (75-85%): æ¸©å’Œæç¤ºï¼Œå¯é€‰å‹ç¼©
        - warning (85-95%): å¼ºçƒˆè­¦å‘Šï¼Œå»ºè®®ç«‹å³å‹ç¼©
        - critical (95%+): å±é™©çŠ¶æ€ï¼Œéœ€è¦å¼ºåˆ¶å‹ç¼©
        """
        context_window = self.get_context_window(model_id)
        usage_ratio = cumulative_prompt_tokens / context_window if context_window > 0 else 0

        # åˆ¤æ–­å“åº”çº§åˆ«
        if usage_ratio < self.context_settings.info_threshold:
            level = "normal"
            needs_compression = False
            message = None

        elif usage_ratio < self.context_settings.warning_threshold:
            level = "info"
            needs_compression = False
            message = self._format_info_message(
                cumulative_prompt_tokens, context_window, usage_ratio
            )

        elif usage_ratio < self.context_settings.critical_threshold:
            level = "warning"
            needs_compression = False
            message = self._format_warning_message(
                cumulative_prompt_tokens, context_window, usage_ratio
            )

        else:  # >= 95%
            level = "critical"
            needs_compression = True
            message = self._format_critical_message(
                cumulative_prompt_tokens, context_window, usage_ratio
            )

        return ContextStatus(
            cumulative_prompt_tokens=cumulative_prompt_tokens,
            context_window=context_window,
            usage_ratio=usage_ratio,
            level=level,
            needs_compression=needs_compression,
            message=message
        )


    def _format_info_message(self, current: int, total: int, ratio: float) -> str:
        """75-85%: æ¸©å’Œæç¤º"""
        return f"""<system_reminder>
ğŸ’¡ Token ä½¿ç”¨æç¤º

å½“å‰ç´¯ç§¯: {current:,} / {total:,} tokens ({ratio:.1%})

å¦‚æœå¯¹è¯è¿˜å°†ç»§ç»­ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨ compact_context å·¥å…·å‹ç¼©ä¸Šä¸‹æ–‡ã€‚
</system_reminder>"""

    def _format_warning_message(self, current: int, total: int, ratio: float) -> str:
        """85-95%: å¼ºçƒˆè­¦å‘Š"""
        return f"""<system_reminder>
âš ï¸ Token ä½¿ç”¨è­¦å‘Š

å½“å‰ç´¯ç§¯: {current:,} / {total:,} tokens ({ratio:.1%})

âš ï¸ å¼ºçƒˆå»ºè®®ç«‹å³ä½¿ç”¨ compact_context å·¥å…·å‹ç¼©ä¸Šä¸‹æ–‡ï¼
å¦‚æœä¸å‹ç¼©ï¼Œå¯¹è¯å¯èƒ½å¾ˆå¿«ä¸­æ–­ã€‚
</system_reminder>"""

    def _format_critical_message(self, current: int, total: int, ratio: float) -> str:
        """95%+: å±é™©çŠ¶æ€ï¼ˆé€šå¸¸ä¸ä¼šæ˜¾ç¤ºç»™ LLMï¼Œå› ä¸ºä¼šè‡ªåŠ¨å‹ç¼©ï¼‰"""
        return f"""<system_reminder>
ğŸš¨ Token ä½¿ç”¨ä¸¥é‡è­¦å‘Š

å½“å‰ç´¯ç§¯: {current:,} / {total:,} tokens ({ratio:.1%})

ğŸš¨ å·²è¾¾åˆ°ä¸´ç•Œé˜ˆå€¼ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨å‹ç¼©ä¸Šä¸‹æ–‡ä»¥é¿å…å¯¹è¯ä¸­æ–­ã€‚
</system_reminder>"""
