"""
ä¸Šä¸‹æ–‡å‹ç¼©å™¨

è´Ÿè´£ï¼š
1. åˆ†å±‚æ¶ˆæ¯ï¼ˆRecent/Middle/Oldï¼‰
2. è°ƒç”¨ LLM æ‰§è¡Œ Compact/Summarize
3. ç”Ÿæˆå‹ç¼©æŠ¥å‘Š
4. é™çº§ç­–ç•¥ï¼ˆå‹ç¼©å¤±è´¥æ—¶ä½¿ç”¨ç®€å•æˆªæ–­ï¼‰
"""

from typing import List, Dict, Literal, Optional, Callable
from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage, ToolMessage
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)


@dataclass
class CompressionResult:
    """å‹ç¼©ç»“æœ"""
    messages: List[BaseMessage]
    before_count: int
    after_count: int
    before_tokens: int  # ç²—ç•¥ä¼°ç®—
    after_tokens: int   # ç²—ç•¥ä¼°ç®—
    strategy: Literal["compact", "summarize", "emergency_truncate"]
    compression_ratio: float


# ===== Prompt æ¨¡æ¿ =====

COMPACT_PROMPT = """ä½ çš„ä»»åŠ¡æ˜¯ä¸ºä¸€ä¸ªé€šç”¨ AI åŠ©æ‰‹çš„å¯¹è¯å†å²åˆ›å»ºè¯¦ç»†æ‘˜è¦ã€‚

**æ‘˜è¦è¦æ±‚ï¼š**
è¯·æŒ‰æ—¶é—´é¡ºåºåˆ†æå¯¹è¯ï¼Œæå–ä»¥ä¸‹å…³é”®ä¿¡æ¯ï¼š

1. **ç”¨æˆ·è¯·æ±‚å’Œæ„å›¾**
   - æ˜ç¡®è®°å½•ç”¨æˆ·çš„æ‰€æœ‰è¯·æ±‚å’Œæ„å›¾

2. **å…³é”®ä¿¡æ¯**
   - æåˆ°çš„é‡è¦æ¦‚å¿µã€ä¸“ä¸šæœ¯è¯­
   - æ•°æ®ã€äº‹å®ã€æ—¶é—´ç‚¹

3. **æ–‡ä»¶æ“ä½œ**
   - æåˆ°çš„æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ `uploads/report.pdf`, `outputs/result.txt`ï¼‰
   - æ–‡ä»¶å†…å®¹æ‘˜è¦ã€æ“ä½œåŸå› 

4. **å·¥å…·è°ƒç”¨è®°å½•**
   - è®°å½•å·¥å…·è°ƒç”¨åŠç»“æœ
   - æ ¼å¼ï¼š`å·¥å…·å(å‚æ•°) â†’ ç»“æœ`

5. **æŠ€èƒ½ä½¿ç”¨**
   - ä½¿ç”¨çš„æŠ€èƒ½ï¼ˆå¦‚ @pdf, @docxï¼‰åŠç”¨é€”

6. **é”™è¯¯å’Œä¿®å¤**
   - é‡åˆ°çš„é”™è¯¯ã€é—®é¢˜
   - è§£å†³æ–¹æ³•å’Œç”¨æˆ·åé¦ˆ

7. **å½“å‰å·¥ä½œ**
   - æœ€æ–°çš„å·¥ä½œè¿›å±•
   - å¾…å®Œæˆçš„äº‹é¡¹ï¼ˆç”¨æˆ·æ˜ç¡®æåˆ°çš„ï¼‰

**è¾“å‡ºæ ¼å¼ï¼š**
è¯·ä½¿ç”¨ä»¥ä¸‹ç»“æ„æä¾›æ‘˜è¦ï¼š

## ç”¨æˆ·è¯·æ±‚å’Œæ„å›¾
[è¯¦ç»†æè¿°]

## å…³é”®ä¿¡æ¯
- [ä¿¡æ¯ 1]
- [ä¿¡æ¯ 2]
...

## æ–‡ä»¶æ“ä½œ
- **æ–‡ä»¶è·¯å¾„ 1**
  - æ“ä½œåŸå› : ...
  - æ›´æ”¹æ‘˜è¦: ...
- **æ–‡ä»¶è·¯å¾„ 2**
  ...

## å·¥å…·è°ƒç”¨è®°å½•
- `tool_name(args)` â†’ ç»“æœ
  - åŸå› : ...
  - å½±å“: ...

## æŠ€èƒ½ä½¿ç”¨
- @skill_name: ç”¨é€”è¯´æ˜

## é”™è¯¯å’Œä¿®å¤
- **é”™è¯¯æè¿°**: ...
  - ä¿®å¤æ–¹æ³•: ...
  - ç”¨æˆ·åé¦ˆ: ...

## å½“å‰å·¥ä½œ
[è¯¦ç»†æè¿°å½“å‰æ­£åœ¨è¿›è¡Œçš„å·¥ä½œ]

---

**é‡è¦æç¤ºï¼š**
- ä¿æŒç®€æ´ï¼ˆæ§åˆ¶åœ¨ 2000 å­—ä»¥å†…ï¼‰
- ä¸è¦è¾“å‡º TODO åˆ—è¡¨ï¼ˆç³»ç»Ÿä¼šåŠ¨æ€è¿½è¸ªï¼‰
- ä»…è¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œä¸è¦åŒ…å«é¢å¤–è¯´æ˜æˆ–å…ƒæ•°æ®

è¯·å¼€å§‹æ€»ç»“ï¼š
"""



class ContextCompressor:
    """ä¸Šä¸‹æ–‡å‹ç¼©å™¨"""

    def __init__(self, settings):
        self.settings = settings
        self.context_settings = settings.context

    async def compress_messages(
        self,
        messages: List[BaseMessage],
        model_invoker: Callable,  # ç”¨äºè°ƒç”¨ LLM çš„å‡½æ•°
        context_window: int = 128000  # æ¨¡å‹çš„ context window
    ) -> CompressionResult:
        """
        æ‰§è¡Œæ¶ˆæ¯å‹ç¼©

        Args:
            messages: å¾…å‹ç¼©çš„æ¶ˆæ¯åˆ—è¡¨
            model_invoker: LLM è°ƒç”¨å‡½æ•°
            context_window: æ¨¡å‹çš„ context window å¤§å°

        Returns:
            CompressionResult åŒ…å«å‹ç¼©åçš„æ¶ˆæ¯å’Œè¯¦ç»†æŠ¥å‘Š
        """
        logger.info("Starting context compression")

        # 2. è®°å½•å‹ç¼©å‰çŠ¶æ€
        before_count = len(messages)
        before_tokens = self._estimate_tokens(messages)

        # 3. åˆ†å±‚æ¶ˆæ¯
        partitioned = self._partition_messages(messages, context_window)

        # 4. æ‰§è¡Œå‹ç¼©
        try:
            compressed = await self._compress_partitioned(
                partitioned,
                model_invoker
            )
        except Exception as e:
            logger.error(f"LLM compression failed: {e}")
            # é™çº§ï¼šä½¿ç”¨ç®€å•æˆªæ–­
            logger.warning("Falling back to simple truncation")
            from .truncator import MessageTruncator
            truncator = MessageTruncator(self.settings)
            compressed = truncator.truncate(messages)
            strategy = "emergency_truncate"
        else:
            strategy = "compact"

        # 5. è®°å½•å‹ç¼©åçŠ¶æ€
        after_count = len(compressed)
        after_tokens = self._estimate_tokens(compressed)
        compression_ratio = after_tokens / before_tokens if before_tokens > 0 else 1.0

        logger.info(
            f"Compression complete: {before_count} â†’ {after_count} messages, "
            f"~{before_tokens} â†’ ~{after_tokens} tokens ({compression_ratio:.1%})"
        )

        return CompressionResult(
            messages=compressed,
            before_count=before_count,
            after_count=after_count,
            before_tokens=before_tokens,
            after_tokens=after_tokens,
            strategy=strategy,
            compression_ratio=compression_ratio
        )

    def _partition_messages(
        self,
        messages: List[BaseMessage],
        context_window: int
    ) -> Dict[str, List[BaseMessage]]:
        """
        åˆ’åˆ†æ¶ˆæ¯ï¼ˆæ··åˆç­–ç•¥ï¼šToken æ¯”ä¾‹ + æ¶ˆæ¯æ•°ï¼‰

        ç­–ç•¥ï¼š
        - System: ä¿ç•™æ‰€æœ‰ SystemMessage
        - Recent: ä¿ç•™æœ€è¿‘ N% context window æˆ– M æ¡æ¶ˆæ¯ï¼ˆå–å…ˆåˆ°è€…ï¼‰
        - Old: å‰©ä½™æ‰€æœ‰æ¶ˆæ¯ï¼ˆå°†è¢«å‹ç¼©ï¼‰
        """
        # 1. åˆ†ç¦» SystemMessage
        system_messages = [m for m in messages if isinstance(m, SystemMessage)]
        non_system_messages = [m for m in messages if not isinstance(m, SystemMessage)]

        # 2. é…ç½®ï¼ˆæ ¹æ® context window è®¡ç®—å®é™… token æ•°ï¼‰
        keep_recent_tokens = int(context_window * self.context_settings.keep_recent_ratio)
        keep_recent_messages = self.context_settings.keep_recent_messages

        logger.debug(
            f"Partition config: keep_recent={keep_recent_tokens} tokens or {keep_recent_messages} msgs "
            f"(context_window={context_window})"
        )

        # 3. ä¼°ç®—æ¯æ¡æ¶ˆæ¯çš„ tokenï¼ˆç²—ç•¥ï¼‰
        message_tokens = [self._estimate_single_message_tokens(m) for m in non_system_messages]

        # 4. ä»åå¾€å‰æ‰«æï¼Œåˆ’åˆ† Recent
        recent_tokens = 0
        recent_count = 0
        for i in range(len(non_system_messages) - 1, -1, -1):
            recent_tokens += message_tokens[i]
            recent_count += 1

            # è¾¾åˆ°ä»»ä¸€æ¡ä»¶å°±åœæ­¢
            if recent_tokens >= keep_recent_tokens or recent_count >= keep_recent_messages:
                break

        recent = non_system_messages[-recent_count:] if recent_count > 0 else []
        old = non_system_messages[:-recent_count] if recent_count > 0 else non_system_messages

        old_tokens = sum(message_tokens[:len(old)]) if old else 0

        logger.debug(
            f"Partitioned messages: system={len(system_messages)}, "
            f"old={len(old)} (~{old_tokens} tokens), "
            f"recent={len(recent)} (~{recent_tokens} tokens)"
        )

        return {
            "system": system_messages,
            "old": old,
            "middle": [],  # ä¿æŒå…¼å®¹æ€§ï¼Œä½†ä¸ºç©º
            "recent": recent
        }

    def _estimate_single_message_tokens(self, msg: BaseMessage) -> int:
        """ä¼°ç®—å•æ¡æ¶ˆæ¯çš„ tokenï¼ˆç²—ç•¥ï¼‰

        ä½¿ç”¨ç®€å•çš„å­—ç¬¦æ•°ä¼°ç®—ï¼š
        - ä¸­æ–‡å¹³å‡ 1 token â‰ˆ 2 chars
        - è‹±æ–‡å¹³å‡ 1 token â‰ˆ 4 chars
        - å–å¹³å‡å€¼: 1 token â‰ˆ 2 chars
        """
        content_len = len(str(msg.content))
        return content_len // 2

    async def _compress_partitioned(
        self,
        partitioned: Dict[str, List[BaseMessage]],
        model_invoker: Callable
    ) -> List[BaseMessage]:
        """
        å‹ç¼©åˆ†å±‚åçš„æ¶ˆæ¯

        ç­–ç•¥ï¼šä¸€æ¬¡æ€§å‹ç¼© Old + Middleï¼Œåªä¿ç•™ Recent
        """
        compressed = []

        # 1. ä¿ç•™ SystemMessage
        compressed.extend(partitioned["system"])

        # 2. åˆå¹¶ Old + Middleï¼Œä¸€æ¬¡æ€§å‹ç¼©
        messages_to_compress = partitioned["old"] + partitioned["middle"]

        if messages_to_compress:
            logger.info(f"Compressing {len(messages_to_compress)} messages (Old + Middle) in single LLM call")
            summary = await self._summarize_messages(
                messages_to_compress,
                model_invoker
            )
            compressed.append(SystemMessage(content=f"""# å¯¹è¯å†å²æ‘˜è¦ï¼ˆç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆï¼‰

ä»¥ä¸‹æ˜¯æ—©æœŸå¯¹è¯çš„æ‘˜è¦ï¼ˆåŸå§‹ {len(messages_to_compress)} æ¡æ¶ˆæ¯ï¼‰ï¼š

{summary}

---
ğŸ“ æœ¬æ¶ˆæ¯ç”±ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆï¼Œç”¨äºèŠ‚çœ tokenã€‚
"""))

        # 3. ä¿ç•™ Recentï¼ˆå®Œæ•´ï¼‰ï¼Œä½†éœ€è¦æ¸…ç†å­¤å„¿ ToolMessage
        recent_messages = partitioned["recent"]
        cleaned_recent = self._clean_orphan_tool_messages(recent_messages)
        compressed.extend(cleaned_recent)

        return compressed

    def _clean_orphan_tool_messages(self, messages: List[BaseMessage]) -> List[BaseMessage]:
        """
        æ¸…ç†å­¤å„¿ ToolMessageï¼ˆæ²¡æœ‰å¯¹åº” tool_call çš„ ToolMessageï¼‰

        åœ¨å‹ç¼©åï¼Œå¦‚æœ AIMessage (åŒ…å« tool_calls) è¢«å‹ç¼©æ‰äº†ï¼Œ
        ä½†å¯¹åº”çš„ ToolMessage è¢«ä¿ç•™åœ¨ Recent ä¸­ï¼Œä¼šå¯¼è‡´ API é”™è¯¯ã€‚

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨

        Returns:
            æ¸…ç†åçš„æ¶ˆæ¯åˆ—è¡¨
        """
        if not messages:
            return messages

        # æ”¶é›†æ‰€æœ‰ tool_call_id
        valid_tool_call_ids = set()
        for msg in messages:
            if isinstance(msg, AIMessage) and hasattr(msg, 'tool_calls') and msg.tool_calls:
                for tc in msg.tool_calls:
                    if 'id' in tc:
                        valid_tool_call_ids.add(tc['id'])

        # è¿‡æ»¤æ‰å­¤å„¿ ToolMessage
        cleaned = []
        for msg in messages:
            if isinstance(msg, ToolMessage):
                # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„ tool_call_id
                tool_call_id = getattr(msg, 'tool_call_id', None)
                if tool_call_id and tool_call_id in valid_tool_call_ids:
                    cleaned.append(msg)
                else:
                    logger.debug(f"Removing orphan ToolMessage: tool_call_id={tool_call_id}")
            else:
                cleaned.append(msg)

        return cleaned

    async def _summarize_messages(
        self,
        messages: List[BaseMessage],
        model_invoker: Callable
    ) -> str:
        """
        ä½¿ç”¨ LLM ç”Ÿæˆæ‘˜è¦

        Args:
            messages: å¾…æ‘˜è¦çš„æ¶ˆæ¯
            model_invoker: LLM è°ƒç”¨å‡½æ•°ï¼ˆæ¥å— prompt å’Œ max_tokensï¼‰

        Returns:
            æ‘˜è¦æ–‡æœ¬
        """
        # æ„é€ è¾“å…¥
        messages_text = self._format_messages_for_summary(messages)
        full_prompt = f"{COMPACT_PROMPT}\n\n{messages_text}"

        # è°ƒç”¨ LLMï¼ˆé™åˆ¶è¾“å‡ºé•¿åº¦ä¸º 2000 å­—ï¼‰
        # ä¸­æ–‡: 1 token â‰ˆ 1.5-2 å­—ç¬¦ï¼Œ2000 å­— â‰ˆ 1200 tokens
        # åŠ  20% buffer: 1200 * 1.2 = 1440 tokens
        summary = await model_invoker(full_prompt, max_tokens=1440)

        return summary.strip()

    def _format_messages_for_summary(self, messages: List[BaseMessage]) -> str:
        """å°†æ¶ˆæ¯æ ¼å¼åŒ–ä¸ºæ–‡æœ¬ï¼ˆä¾› LLM æ‘˜è¦ï¼‰"""
        formatted = []

        for msg in messages:
            role = msg.__class__.__name__.replace("Message", "")
            content = str(msg.content)[:2000]  # é™åˆ¶é•¿åº¦

            if isinstance(msg, AIMessage) and hasattr(msg, 'tool_calls') and msg.tool_calls:
                tools = ", ".join(tc.get("name", "unknown") for tc in msg.tool_calls)
                formatted.append(f"[{role}] è°ƒç”¨å·¥å…·: {tools}")
            elif isinstance(msg, ToolMessage):
                tool_name = getattr(msg, 'name', 'unknown')
                formatted.append(f"[{role}:{tool_name}] {content[:500]}...")
            else:
                formatted.append(f"[{role}] {content}")

        return "\n\n".join(formatted)

    def _estimate_tokens(self, messages: List[BaseMessage]) -> int:
        """
        ç²—ç•¥ä¼°ç®— token æ•°

        ä½¿ç”¨ç®€å•çš„å­—ç¬¦æ•°ä¼°ç®—ï¼š
        - ä¸­æ–‡: ~1.5 chars/token
        - è‹±æ–‡: ~4 chars/token
        - å¹³å‡: ~2 chars/token
        """
        total_chars = sum(len(str(m.content)) for m in messages)
        return total_chars // 2  # ç²—ç•¥ä¼°ç®—
