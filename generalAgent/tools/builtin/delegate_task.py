"""Delegate complex tasks to an isolated agent - Claude Code style."""

from __future__ import annotations

import asyncio
import json
import uuid
from typing import Any, Optional, Annotated

from langchain_core.messages import HumanMessage
from langchain_core.tools import tool, InjectedToolArg
from langgraph.types import Command

# Module-level variables to store app graph and parent state (set by runtime/planner)
# Changed from ContextVar to simple module variable to avoid async context issues
_app_graph: Optional[Any] = None
_parent_state_store: dict[str, dict] = {}  # {thread_id: parent_state}


def set_app_graph(app_graph):
    """Set the application graph for delegated task execution.

    Called by runtime after graph is built.
    """
    global _app_graph
    _app_graph = app_graph


def set_parent_state(thread_id: str, state: dict):
    """Store parent state for subagent inheritance.

    Called by planner before tool execution.
    """
    global _parent_state_store
    _parent_state_store[thread_id] = state


@tool
async def delegate_task(
    task: str,
    max_loops: int = 50,
    config: Annotated[dict, InjectedToolArg] = None,
) -> str:
    """å°†ç‹¬ç«‹å­ä»»åŠ¡å§”æ´¾ç»™ä¸“ç”¨å­ agent æ‰§è¡Œï¼ˆé€‚åˆéœ€è¦å¤šè½®è¿­ä»£çš„ä»»åŠ¡ï¼‰

    âš ï¸ **é‡è¦ï¼šå­ agent ç»§æ‰¿ä¸» agent çš„å·¥å…·å’ŒæŠ€èƒ½**
    - å­ agent çœ‹ä¸åˆ°ä¸»å¯¹è¯å†å²ï¼ˆç‹¬ç«‹ä¸Šä¸‹æ–‡ï¼‰

    **ä½•æ—¶ä½¿ç”¨ï¼š**
    - éœ€è¦å¤šè½®å·¥å…·è°ƒç”¨çš„å¤æ‚å­ä»»åŠ¡ï¼ˆæ·±åº¦ç ”ç©¶ã€åå¤å°è¯•ã€å¤§æ–‡æ¡£åˆ†æï¼‰
    - å¯èƒ½äº§ç”Ÿå¤§é‡ä¸­é—´ç»“æœçš„ä»»åŠ¡ï¼ˆç½‘é¡µæœç´¢ã€å¤šæ¬¡æœç´¢ã€æ‰¹é‡æ–‡ä»¶å¤„ç†ï¼‰ï¼Œé¿å…æ±¡æŸ“ä¸»å¯¹è¯

    **ä»»åŠ¡æè¿°è¦æ±‚ï¼š**
    å¿…é¡»åŒ…å«ï¼š
    1. ç›®æ ‡æ˜¯ä»€ä¹ˆ
    2. éœ€è¦å“ªäº›ä¸Šä¸‹æ–‡ä¿¡æ¯
    3. æœŸæœ›çš„è¿”å›æ ¼å¼ï¼ˆMarkdown è¡¨æ ¼ã€JSONã€æ–‡æœ¬æ‘˜è¦ç­‰ï¼‰

    Args:
        task: è¯¦ç»†çš„ä»»åŠ¡æè¿°ï¼ˆå¿…é¡»è‡ªåŒ…å«ï¼ï¼‰

    Examples:
        # æ·±åº¦æœç´¢
        delegate_task("æœç´¢ src/ ç›®å½•ä¸‹æ‰€æœ‰ä½¿ç”¨ old_api() çš„ä»£ç ã€‚"
                      "è¦æ±‚ï¼šè®°å½•æ–‡ä»¶è·¯å¾„ã€è¡Œå·ã€è°ƒç”¨ä¸Šä¸‹æ–‡ã€‚"
                      "è¿”å›ï¼šMarkdown è¡¨æ ¼ [æ–‡ä»¶ | è¡Œå· | ä»£ç ç‰‡æ®µ]")

        # åå¤è°ƒè¯•
        delegate_task("è¿è¡Œè„šæœ¬ scripts/migrate.pyï¼Œå¦‚æœå‡ºé”™åˆ™åˆ†æå¹¶ä¿®å¤ï¼Œé‡å¤ç›´åˆ°æˆåŠŸã€‚"
                      "è¿”å›ï¼š1) æœ€ç»ˆå¯è¿è¡Œçš„ä»£ç ï¼Œ2) é‡åˆ°çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ")

        # å¤§æ–‡æ¡£åˆ†æ
        delegate_task("åˆ†æ uploads/report.pdfï¼ˆ80é¡µï¼‰ï¼š"
                      "1) æå–æ‰€æœ‰è¡¨æ ¼æ•°æ®"
                      "2) è®¡ç®—å…³é”®æŒ‡æ ‡ï¼ˆæ”¶å…¥ã€æ”¯å‡ºã€åˆ©æ¶¦ï¼‰"
                      "è¿”å›ï¼šç»“æ„åŒ– JSON")
    """
    try:
        # Get app graph from module variable
        app_graph = _app_graph
        if app_graph is None:
            return json.dumps({
                "ok": False,
                "error": "Application graph not initialized",
            }, ensure_ascii=False)

        # Get parent state from config (injected by LangGraph)
        parent_state = {}
        parent_thread_id = None
        if config:
            configurable = config.get("configurable", {})
            parent_thread_id = configurable.get("thread_id")
            if parent_thread_id and parent_thread_id in _parent_state_store:
                parent_state = _parent_state_store[parent_thread_id]

        # Generate unique context ID
        context_id = f"subagent-{uuid.uuid4().hex[:8]}"

        # Inherit from parent state
        parent_mentioned_agents = parent_state.get("mentioned_agents", [])
        parent_active_skill = parent_state.get("active_skill")
        parent_workspace = parent_state.get("workspace_path")
        parent_uploaded_files = parent_state.get("uploaded_files", [])

        # Create independent state for delegated agent
        delegated_state = {
            "messages": [HumanMessage(content=task)],
            "images": [],
            "active_skill": parent_active_skill,  # Inherit active skill
            "allowed_tools": [],
            "mentioned_agents": list(parent_mentioned_agents),  # Inherit @mentions
            "new_mentioned_agents": [],  # No new mentions initially
            "persistent_tools": [],
            "model_pref": None,
            "todos": [],
            "context_id": context_id,
            "parent_context": parent_state.get("context_id", "main"),
            "loops": 0,
            "max_loops": max_loops,
            "thread_id": context_id,  # Use context_id as thread_id for isolation
            "user_id": parent_state.get("user_id"),
            "workspace_path": parent_workspace,  # Inherit workspace
            "uploaded_files": list(parent_uploaded_files),  # Inherit uploaded files
            "new_uploaded_files": [],  # No new uploads initially
        }

        # Run delegated agent in isolated context with streaming
        config = {"configurable": {"thread_id": context_id}}

        print(f"\n[subagent-{context_id[:8]}] Starting execution...")

        final_state = None
        message_count = 1  # Start at 1 (user message already there)

        # Use astream for real-time output with interrupt handling
        async for state_snapshot in app_graph.astream(
            delegated_state,
            config=config,
            stream_mode="values"
        ):
            final_state = state_snapshot

            # Print new messages
            current_messages = state_snapshot.get("messages", [])
            for idx in range(message_count, len(current_messages)):
                msg = current_messages[idx]

                # Determine message type and content
                if hasattr(msg, "content"):
                    content = str(msg.content)
                    if hasattr(msg, "type"):
                        msg_type = msg.type
                    else:
                        msg_type = msg.__class__.__name__

                    # Print based on type
                    if msg_type in {"ai", "AIMessage"}:
                        if content:
                            print(f"[subagent-{context_id[:8]}] {content}")
                    elif msg_type in {"tool", "ToolMessage"}:
                        # Print tool calls concisely
                        tool_name = getattr(msg, "name", "tool")
                        if content:
                            print(f"[subagent-{context_id[:8]}] [tool: {tool_name}] {content[:100]}...")

            message_count = len(current_messages)

        # Handle interrupts (e.g., ask_human)
        while True:
            graph_state = await app_graph.aget_state(config)

            # Check if there are any interrupts
            if (graph_state.next and graph_state.tasks and
                hasattr(graph_state.tasks[0], 'interrupts') and
                graph_state.tasks[0].interrupts):

                # Get interrupt data
                interrupt_value = graph_state.tasks[0].interrupts[0].value
                interrupt_type = interrupt_value.get("type", "generic")

                if interrupt_type == "user_input_request":
                    # Handle ask_human request
                    question = interrupt_value.get("question", "")
                    context_info = interrupt_value.get("context", "")
                    default = interrupt_value.get("default")

                    # Print question with subagent prefix
                    print()
                    if context_info:
                        print(f"[subagent-{context_id[:8]}] ğŸ’¡ {context_info}")
                    print(f"[subagent-{context_id[:8]}] ğŸ’¬ {question}")
                    if default:
                        print(f"[subagent-{context_id[:8]}]    (é»˜è®¤: {default})")

                    # Get user input (synchronous in async context)
                    loop = asyncio.get_event_loop()
                    answer = await loop.run_in_executor(None, lambda: input("> ").strip())

                    # Handle empty answer
                    if not answer and default:
                        answer = default
                        print(f"[subagent-{context_id[:8]}] âœ“ ä½¿ç”¨é»˜è®¤å€¼: {default}")

                    # Resume execution with answer
                    async for state_snapshot in app_graph.astream(
                        Command(resume=answer),
                        config=config,
                        stream_mode="values"
                    ):
                        final_state = state_snapshot

                        # Print new messages
                        current_messages = state_snapshot.get("messages", [])
                        for idx in range(message_count, len(current_messages)):
                            msg = current_messages[idx]
                            if hasattr(msg, "content"):
                                content = str(msg.content)
                                msg_type = getattr(msg, "type", msg.__class__.__name__)
                                if msg_type in {"ai", "AIMessage"} and content:
                                    print(f"[subagent-{context_id[:8]}] {content}")

                        message_count = len(current_messages)
                else:
                    # Unknown interrupt type, skip
                    print(f"[subagent-{context_id[:8]}] âš ï¸ Unknown interrupt type: {interrupt_type}")
                    break
            else:
                # No more interrupts, execution complete
                break

        print(f"[subagent-{context_id[:8]}] Completed\n")

        # Extract result from final message
        if final_state:
            messages = final_state.get("messages", [])
            if messages:
                last_message = messages[-1]
                result_text = getattr(last_message, "content", "No response")
            else:
                result_text = "No response from delegated agent"

            # Check if result is too brief (< 200 chars), request more detailed summary (max 1 retry)
            if len(result_text) < 200:
                print(f"[subagent-{context_id[:8]}] âš ï¸ ç»“æœå¤ªç®€çŸ­ï¼ˆ{len(result_text)} charsï¼‰ï¼Œè¯·æ±‚æ›´è¯¦ç»†çš„æ‘˜è¦...\n")

                # Create continuation prompt
                continuation_prompt = HumanMessage(content="""ä½ çš„ä¸Šä¸€æ¬¡å›å¤å¤ªç®€çŸ­äº†ï¼ˆ< 200 å­—ç¬¦ï¼‰ã€‚

è¯·æä¾›æ›´è¯¦ç»†çš„æ‘˜è¦ï¼ŒåŒ…æ‹¬ï¼š
1. ä½ åšäº†ä»€ä¹ˆï¼ˆä½¿ç”¨äº†å“ªäº›å·¥å…·ï¼Œè¯»å–äº†å“ªäº›æ–‡ä»¶ï¼‰
2. å‘ç°äº†ä»€ä¹ˆï¼ˆå…³é”®ä¿¡æ¯ã€é”™è¯¯ã€è§£å†³æ–¹æ¡ˆï¼‰
3. ç»“æœæ˜¯ä»€ä¹ˆï¼ˆæ–‡ä»¶è·¯å¾„ã€å‡½æ•°åã€é…ç½®ç­‰ï¼‰

**é‡è¦**ï¼šä¸» Agent æ— æ³•çœ‹åˆ°ä½ çš„å·¥å…·è°ƒç”¨å†å²ï¼Œåªèƒ½çœ‹åˆ°ä½ çš„æœ€ç»ˆå›å¤ï¼""")

                # Continue execution with the continuation prompt
                message_count = len(messages)  # Reset counter for continuation
                async for state_snapshot in app_graph.astream(
                    {**final_state, "messages": messages + [continuation_prompt]},
                    config=config,
                    stream_mode="values"
                ):
                    final_state = state_snapshot

                    # Print new messages
                    current_messages = state_snapshot.get("messages", [])
                    for idx in range(message_count, len(current_messages)):
                        msg = current_messages[idx]
                        if hasattr(msg, "content"):
                            content = str(msg.content)
                            msg_type = getattr(msg, "type", msg.__class__.__name__)
                            if msg_type in {"ai", "AIMessage"} and content:
                                print(f"[subagent-{context_id[:8]}] {content}")

                    message_count = len(current_messages)

                print(f"[subagent-{context_id[:8]}] Continuation completed\n")

                # Re-extract the final result
                messages = final_state.get("messages", [])
                if messages:
                    last_message = messages[-1]
                    result_text = getattr(last_message, "content", "No response")

            return json.dumps({
                "ok": True,
                "result": result_text,
                "context_id": context_id,
                "loops": final_state.get("loops", 0),
            }, ensure_ascii=False)
        else:
            return json.dumps({
                "ok": False,
                "error": "Delegated agent execution produced no final state",
            }, ensure_ascii=False)

    except Exception as e:
        return json.dumps({
            "ok": False,
            "error": f"Delegated agent execution failed: {str(e)}",
        }, ensure_ascii=False)


__all__ = ["delegate_task", "set_parent_state"]
