"""
ä¸Šä¸‹æ–‡å‹ç¼©ç³»ç»Ÿçš„å®Œæ•´å•å…ƒæµ‹è¯•

æµ‹è¯•è¦†ç›–ï¼š
1. Token ç›‘æ§å’Œè­¦å‘Šè§¦å‘
2. åˆ†å±‚é€»è¾‘ï¼ˆæ··åˆç­–ç•¥ï¼šToken æ¯”ä¾‹ + æ¶ˆæ¯æ•°ï¼‰
3. å‹ç¼©æ‰§è¡Œå’Œç»“æœç»“æ„
4. é™çº§ç­–ç•¥
"""

import pytest
from unittest.mock import AsyncMock, Mock, patch
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage

from generalAgent.context.token_tracker import TokenTracker, ContextStatus, TokenUsage
from generalAgent.context.compressor import ContextCompressor, CompressionResult
from generalAgent.context.manager import ContextManager
from generalAgent.config.settings import get_settings


# ========== Fixtures ==========

@pytest.fixture
def settings():
    """è·å–æµ‹è¯•é…ç½®"""
    return get_settings()


@pytest.fixture
def tracker(settings):
    """åˆ›å»º TokenTracker å®ä¾‹"""
    return TokenTracker(settings)


@pytest.fixture
def compressor(settings):
    """åˆ›å»º ContextCompressor å®ä¾‹"""
    return ContextCompressor(settings)


@pytest.fixture
def manager(settings):
    """åˆ›å»º ContextManager å®ä¾‹"""
    return ContextManager(settings)


@pytest.fixture
def sample_messages():
    """åˆ›å»ºæµ‹è¯•æ¶ˆæ¯åˆ—è¡¨ï¼ˆ50 æ¡ï¼‰"""
    messages = [SystemMessage(content="System prompt")]

    for i in range(49):
        messages.append(HumanMessage(content=f"User question {i}"))
        messages.append(AIMessage(content=f"AI response {i}" * 50))  # è¾ƒé•¿çš„å›å¤

    return messages


# ========== Test TokenTracker ==========

class TestTokenTracker:
    """æµ‹è¯• Token ç›‘æ§å™¨"""

    def test_extract_token_usage_from_response_metadata(self, tracker):
        """æµ‹è¯•ä» response_metadata æå– token ä½¿ç”¨é‡"""
        response = AIMessage(
            content="test",
            response_metadata={
                "token_usage": {
                    "prompt_tokens": 1000,
                    "completion_tokens": 500,
                    "total_tokens": 1500
                }
            }
        )

        usage = tracker.extract_token_usage(response)

        assert usage is not None
        assert usage.prompt_tokens == 1000
        assert usage.completion_tokens == 500
        assert usage.total_tokens == 1500

    def test_extract_token_usage_from_usage_subfield(self, tracker):
        """æµ‹è¯•ä» response_metadata.usage æå– token ä½¿ç”¨é‡"""
        response = AIMessage(
            content="test",
            response_metadata={
                "usage": {
                    "prompt_tokens": 2000,
                    "completion_tokens": 800,
                    "total_tokens": 2800
                }
            }
        )

        usage = tracker.extract_token_usage(response)

        assert usage is not None
        assert usage.prompt_tokens == 2000
        assert usage.completion_tokens == 800

    def test_extract_token_usage_no_metadata(self, tracker):
        """æµ‹è¯•æ²¡æœ‰ metadata æ—¶è¿”å› None"""
        response = AIMessage(content="test")

        usage = tracker.extract_token_usage(response)

        assert usage is None

    def test_get_context_window(self, tracker):
        """æµ‹è¯•è·å–æ¨¡å‹ context window"""
        # ç²¾ç¡®åŒ¹é…
        assert tracker.get_context_window("deepseek-chat") == 128000
        assert tracker.get_context_window("kimi-k2-0905-preview") == 200000

        # å‰ç¼€åŒ¹é…
        assert tracker.get_context_window("deepseek-chat-v2") == 128000
        assert tracker.get_context_window("gpt-4o") == 128000

        # æœªçŸ¥æ¨¡å‹è¿”å›é»˜è®¤å€¼
        assert tracker.get_context_window("unknown-model") == 128000

    def test_check_status_normal(self, tracker):
        """æµ‹è¯•æ­£å¸¸çŠ¶æ€ï¼ˆ< 75%ï¼‰"""
        status = tracker.check_status(
            cumulative_prompt_tokens=50000,
            model_id="deepseek-chat"  # 128k
        )

        assert status.level == "normal"
        assert status.usage_ratio == pytest.approx(50000 / 128000)
        assert status.needs_compression is False
        assert status.message is None

    def test_check_status_info(self, tracker):
        """æµ‹è¯•ä¿¡æ¯æç¤ºçŠ¶æ€ï¼ˆ75-85%ï¼‰"""
        status = tracker.check_status(
            cumulative_prompt_tokens=100000,  # 78%
            model_id="deepseek-chat"
        )

        assert status.level == "info"
        assert status.usage_ratio == pytest.approx(100000 / 128000)
        assert status.needs_compression is False
        assert "ğŸ’¡ Token ä½¿ç”¨æç¤º" in status.message
        assert "compact_context å·¥å…·" in status.message

    def test_check_status_warning(self, tracker):
        """æµ‹è¯•è­¦å‘ŠçŠ¶æ€ï¼ˆ85-95%ï¼‰"""
        status = tracker.check_status(
            cumulative_prompt_tokens=115000,  # 89.8%
            model_id="deepseek-chat"
        )

        assert status.level == "warning"
        assert status.usage_ratio == pytest.approx(115000 / 128000)
        assert status.needs_compression is False
        assert "âš ï¸ Token ä½¿ç”¨è­¦å‘Š" in status.message

    def test_check_status_critical(self, tracker):
        """æµ‹è¯•å±é™©çŠ¶æ€ï¼ˆâ‰¥ 95%ï¼‰"""
        status = tracker.check_status(
            cumulative_prompt_tokens=122000,  # 95.3%
            model_id="deepseek-chat"
        )

        assert status.level == "critical"
        assert status.usage_ratio == pytest.approx(122000 / 128000)
        assert status.needs_compression is True
        assert "ğŸš¨ Token ä½¿ç”¨ä¸¥é‡è­¦å‘Š" in status.message


# ========== Test ContextCompressor ==========

class TestContextCompressor:
    """æµ‹è¯•ä¸Šä¸‹æ–‡å‹ç¼©å™¨"""

    def test_estimate_single_message_tokens(self, compressor):
        """æµ‹è¯•å•æ¡æ¶ˆæ¯ token ä¼°ç®—"""
        msg = HumanMessage(content="Hello world" * 100)  # ~1100 chars

        tokens = compressor._estimate_single_message_tokens(msg)

        # 1100 chars / 2 â‰ˆ 550 tokens
        assert tokens == 550

    def test_partition_messages_small_dataset(self, compressor):
        """æµ‹è¯•å°æ•°æ®é›†åˆ†å±‚ï¼ˆä¸è¶³ recent é˜ˆå€¼ï¼‰"""
        messages = [
            SystemMessage(content="System"),
            HumanMessage(content="Q1"),
            AIMessage(content="A1"),
            HumanMessage(content="Q2"),
            AIMessage(content="A2"),
        ]

        partitioned = compressor._partition_messages(messages, context_window=128000)

        assert len(partitioned["system"]) == 1
        assert len(partitioned["old"]) == 0
        assert len(partitioned["middle"]) == 0
        assert len(partitioned["recent"]) == 4  # å…¨éƒ¨åœ¨ recent

    def test_partition_messages_mixed_strategy(self, compressor, sample_messages):
        """æµ‹è¯•æ··åˆç­–ç•¥åˆ†å±‚ï¼ˆToken æ¯”ä¾‹ + æ¶ˆæ¯æ•°ï¼‰"""
        # sample_messages: 1 System + 98 æ¡å¯¹è¯
        # é»˜è®¤é…ç½®ï¼škeep_recent_ratio=0.15, keep_recent_messages=10
        #           compact_middle_ratio=0.30, compact_middle_messages=30

        partitioned = compressor._partition_messages(sample_messages, context_window=128000)

        # éªŒè¯ system
        assert len(partitioned["system"]) == 1

        # éªŒè¯ recentï¼ˆåº”è¯¥è¾¾åˆ° 10 æ¡æˆ– 19.2k tokens çš„é™åˆ¶ï¼‰
        assert len(partitioned["recent"]) <= 10
        assert len(partitioned["recent"]) > 0

        # éªŒè¯ middleï¼ˆåº”è¯¥è¾¾åˆ° 30 æ¡æˆ– 38.4k tokens çš„é™åˆ¶ï¼‰
        assert len(partitioned["middle"]) <= 30

        # éªŒè¯ oldï¼ˆå‰©ä½™æ¶ˆæ¯ï¼‰
        non_system_count = len(sample_messages) - 1
        recent_count = len(partitioned["recent"])
        middle_count = len(partitioned["middle"])
        assert len(partitioned["old"]) == non_system_count - recent_count - middle_count

    def test_partition_messages_with_large_context_window(self, compressor, sample_messages):
        """æµ‹è¯•å¤§ context window çš„åˆ†å±‚ï¼ˆKimi 200kï¼‰"""
        partitioned = compressor._partition_messages(sample_messages, context_window=200000)

        # 200k * 0.15 = 30k tokens (Recent)
        # 200k * 0.30 = 60k tokens (Middle)
        # ç”±äº sample_messages æ€»å…±çº¦ 50k tokensï¼Œåº”è¯¥å¤§éƒ¨åˆ†åœ¨ recent/middle

        assert len(partitioned["system"]) == 1
        assert len(partitioned["recent"]) > 0
        # Old å¯èƒ½ä¸ºç©ºæˆ–å¾ˆå°‘ï¼ˆå› ä¸º context window å¾ˆå¤§ï¼‰

    @pytest.mark.asyncio
    async def test_compress_messages_success(self, compressor, sample_messages):
        """æµ‹è¯•æˆåŠŸå‹ç¼©"""
        # Mock LLM è°ƒç”¨
        async def mock_invoker(prompt, max_tokens=2048):
            return "## ç”¨æˆ·è¯·æ±‚å’Œæ„å›¾\nç”¨æˆ·è¿›è¡Œäº†å¤šè½®å¯¹è¯\n\n## å·¥å…·è°ƒç”¨è®°å½•\næ— "

        result = await compressor.compress_messages(
            messages=sample_messages,
            model_invoker=mock_invoker,
            context_window=128000
        )

        assert result.strategy == "compact"
        assert result.before_count == len(sample_messages)
        assert result.after_count < result.before_count
        assert result.compression_ratio < 1.0

        # éªŒè¯å‹ç¼©åçš„æ¶ˆæ¯ç»“æ„
        assert isinstance(result.messages[0], SystemMessage)  # åŸå§‹ system
        # åº”è¯¥æœ‰ Old/Middle çš„å‹ç¼©æ‘˜è¦ï¼ˆSystemMessageï¼‰
        # åŠ ä¸Š Recent çš„å®Œæ•´æ¶ˆæ¯

    @pytest.mark.asyncio
    async def test_compress_messages_with_max_tokens_limit(self, compressor, sample_messages):
        """æµ‹è¯• max_tokens é™åˆ¶ç”Ÿæ•ˆ"""
        call_args = []

        async def mock_invoker(prompt, max_tokens=2048):
            call_args.append({"prompt": prompt, "max_tokens": max_tokens})
            return "Compressed summary"

        await compressor.compress_messages(
            messages=sample_messages,
            model_invoker=mock_invoker,
            context_window=128000
        )

        # éªŒè¯ max_tokens=1440 è¢«ä¼ é€’
        assert len(call_args) > 0
        for call in call_args:
            assert call["max_tokens"] == 1440

    def test_format_messages_for_summary(self, compressor):
        """æµ‹è¯•æ¶ˆæ¯æ ¼å¼åŒ–"""
        messages = [
            HumanMessage(content="Hello"),
            AIMessage(
                content="Hi",
                tool_calls=[{"name": "test_tool", "args": {}, "id": "call_123"}]
            ),
            ToolMessage(content="Result", name="test_tool", tool_call_id="call_123")
        ]

        formatted = compressor._format_messages_for_summary(messages)

        assert "[Human] Hello" in formatted
        assert "è°ƒç”¨å·¥å…·: test_tool" in formatted
        assert "[Tool:test_tool]" in formatted


# ========== Test ContextManager ==========

class TestContextManager:
    """æµ‹è¯•ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""

    def test_extract_and_check_no_usage(self, manager):
        """æµ‹è¯•æ²¡æœ‰ token ä½¿ç”¨ä¿¡æ¯çš„å“åº”"""
        response = AIMessage(content="test")

        report = manager.extract_and_check(
            response=response,
            cumulative_prompt_tokens=0,
            model_id="deepseek-chat"
        )

        assert report.action == "none"
        assert report.status is None

    def test_extract_and_check_normal(self, manager):
        """æµ‹è¯•æ­£å¸¸çŠ¶æ€"""
        response = AIMessage(
            content="test",
            response_metadata={"token_usage": {"prompt_tokens": 1000, "completion_tokens": 500}}
        )

        report = manager.extract_and_check(
            response=response,
            cumulative_prompt_tokens=50000,
            model_id="deepseek-chat"
        )

        assert report.action == "none"
        assert report.status.level == "normal"

    def test_extract_and_check_warning(self, manager):
        """æµ‹è¯•è­¦å‘ŠçŠ¶æ€"""
        response = AIMessage(
            content="test",
            response_metadata={"token_usage": {"prompt_tokens": 50000, "completion_tokens": 500}}
        )

        report = manager.extract_and_check(
            response=response,
            cumulative_prompt_tokens=60000,  # ç´¯ç§¯åˆ° 110k (85.9%)
            model_id="deepseek-chat"
        )

        assert report.action == "warning"
        assert report.status.level == "warning"
        assert report.user_message is not None

    @pytest.mark.asyncio
    async def test_compress_context_success(self, manager, sample_messages):
        """æµ‹è¯•å‹ç¼©æˆåŠŸ"""
        async def mock_invoker(prompt, max_tokens=2048):
            return "Compressed content"

        result = await manager.compress_context(
            messages=sample_messages,
            model_invoker=mock_invoker,
            context_window=128000
        )

        assert result.strategy == "compact"
        assert result.after_count < result.before_count

    @pytest.mark.asyncio
    async def test_compress_context_fallback_on_error(self, manager, sample_messages):
        """æµ‹è¯•å‹ç¼©å¤±è´¥æ—¶é™çº§"""
        async def mock_invoker_fail(prompt, max_tokens=2048):
            raise RuntimeError("LLM call failed")

        result = await manager.compress_context(
            messages=sample_messages,
            model_invoker=mock_invoker_fail,
            context_window=128000
        )

        # åº”è¯¥é™çº§åˆ° emergency_truncate
        assert result.strategy == "emergency_truncate"
        assert result.after_count == min(len(sample_messages), 100)  # é»˜è®¤ä¿ç•™ 100 æ¡

    def test_format_compression_report(self, manager):
        """æµ‹è¯•å‹ç¼©æŠ¥å‘Šæ ¼å¼åŒ–"""
        result = CompressionResult(
            messages=[SystemMessage(content="test")],
            before_count=141,
            after_count=14,
            before_tokens=105000,
            after_tokens=18000,
            strategy="compact",
            compression_ratio=18000 / 105000
        )

        report = manager.format_compression_report(result)

        assert "âœ… ä¸Šä¸‹æ–‡å·²å‹ç¼©" in report
        assert "141 æ¡æ¶ˆæ¯" in report
        assert "14 æ¡æ¶ˆæ¯" in report
        assert "è¯¦ç»†æ‘˜è¦" in report
        assert "127 æ¡æ¶ˆæ¯" in report  # èŠ‚çœçš„æ¶ˆæ¯æ•°


# ========== Integration Test: Full Compression Flow ==========

class TestCompressionIntegration:
    """é›†æˆæµ‹è¯•ï¼šå®Œæ•´å‹ç¼©æµç¨‹"""

    @pytest.mark.asyncio
    async def test_full_compression_workflow(self, manager, sample_messages):
        """æµ‹è¯•å®Œæ•´å‹ç¼©å·¥ä½œæµ"""
        # æ¨¡æ‹Ÿ LLM è°ƒç”¨ï¼Œè¿”å›ç»“æ„åŒ–æ‘˜è¦
        async def mock_llm_invoker(prompt, max_tokens=2048):
            return """## ç”¨æˆ·è¯·æ±‚å’Œæ„å›¾
ç”¨æˆ·è¿›è¡Œäº†å¤šè½®é—®ç­”

## å…³é”®ä¿¡æ¯
- æµ‹è¯•å¯¹è¯
- å¤šæ¬¡äº¤äº’

## å·¥å…·è°ƒç”¨è®°å½•
æ— 

## å½“å‰å·¥ä½œ
å¯¹è¯è¿›è¡Œä¸­"""

        # æ‰§è¡Œå‹ç¼©
        result = await manager.compress_context(
            messages=sample_messages,
            model_invoker=mock_llm_invoker,
            context_window=128000
        )

        # éªŒè¯ç»“æœ
        assert result.strategy == "compact"
        assert result.compression_ratio < 0.3  # åº”è¯¥å‹ç¼©åˆ° < 30%

        # éªŒè¯å‹ç¼©åçš„æ¶ˆæ¯ç»“æ„
        compressed_messages = result.messages

        # ç¬¬ä¸€æ¡åº”è¯¥æ˜¯åŸå§‹ SystemMessage
        assert isinstance(compressed_messages[0], SystemMessage)
        assert compressed_messages[0].content == "System prompt"

        # åç»­åº”è¯¥æœ‰å‹ç¼©æ‘˜è¦ï¼ˆSystemMessageï¼‰
        summary_messages = [
            m for m in compressed_messages[1:]
            if isinstance(m, SystemMessage) and "æ‘˜è¦" in m.content
        ]
        assert len(summary_messages) >= 1  # è‡³å°‘æœ‰ Old æˆ– Middle çš„æ‘˜è¦

        # æœ€ååº”è¯¥æœ‰ Recent çš„å®Œæ•´æ¶ˆæ¯
        recent_messages = [
            m for m in compressed_messages
            if not isinstance(m, SystemMessage)
        ]
        assert len(recent_messages) > 0
        assert len(recent_messages) <= 10  # é»˜è®¤ä¿ç•™ 10 æ¡

    @pytest.mark.asyncio
    async def test_compressed_context_structure(self, compressor):
        """æµ‹è¯•å‹ç¼©åçš„ä¸Šä¸‹æ–‡ç»“æ„"""
        # åˆ›å»ºå¤§é‡æ¶ˆæ¯
        messages = [SystemMessage(content="System")]
        for i in range(100):
            messages.append(HumanMessage(content=f"Q{i}"))
            messages.append(AIMessage(content=f"A{i}" * 100))  # é•¿å›å¤

        async def mock_invoker(prompt, max_tokens=2048):
            # è¿”å›ç¬¦åˆè¦æ±‚çš„ç»“æ„åŒ–æ‘˜è¦
            return """## ç”¨æˆ·è¯·æ±‚å’Œæ„å›¾
æµ‹è¯•

## å…³é”®ä¿¡æ¯
- æµ‹è¯•æ•°æ®

## æ–‡ä»¶æ“ä½œ
æ— 

## å·¥å…·è°ƒç”¨è®°å½•
æ— 

## å½“å‰å·¥ä½œ
æµ‹è¯•ä¸­"""

        result = await compressor.compress_messages(
            messages=messages,
            model_invoker=mock_invoker,
            context_window=128000
        )

        # éªŒè¯ç»“æ„
        compressed = result.messages

        # 1. ç¬¬ä¸€æ¡æ˜¯åŸå§‹ SystemMessage
        assert compressed[0].content == "System"

        # 2. ä¸­é—´æœ‰å‹ç¼©æ‘˜è¦
        summary_count = sum(
            1 for m in compressed[1:]
            if isinstance(m, SystemMessage) and "æ‘˜è¦" in m.content
        )
        assert summary_count >= 1

        # 3. æ‘˜è¦å†…å®¹åŒ…å«å¿…éœ€ç« èŠ‚
        for msg in compressed[1:]:
            if isinstance(msg, SystemMessage) and "æ‘˜è¦" in msg.content:
                assert "ç”¨æˆ·è¯·æ±‚" in msg.content or "å…³é”®ä¿¡æ¯" in msg.content
                assert "ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ" in msg.content

        # 4. Recent æ¶ˆæ¯ä¿æŒå®Œæ•´
        recent_start_idx = None
        for i, msg in enumerate(compressed):
            if not isinstance(msg, SystemMessage):
                recent_start_idx = i
                break

        if recent_start_idx:
            recent_messages = compressed[recent_start_idx:]
            assert len(recent_messages) <= 10
            # éªŒè¯å†…å®¹æœªè¢«å‹ç¼©
            assert any("Q" in str(m.content) for m in recent_messages)


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
