"""
Unit æµ‹è¯•ï¼šè‡ªåŠ¨å‹ç¼©æ ¸å¿ƒé€»è¾‘

æµ‹è¯•èŒƒå›´ï¼š
1. Token tracker æ­£ç¡®è¯†åˆ« critical çŠ¶æ€
2. ContextManager å‹ç¼©é€»è¾‘
3. State æ›´æ–°é€»è¾‘
4. é˜²é‡å¤å‹ç¼©æ ‡å¿—

ä¸æ¶‰åŠï¼š
- å®Œæ•´çš„ planner node
- LLM è°ƒç”¨
- å®Œæ•´çš„ LangGraph æµç¨‹
"""

import pytest
from unittest.mock import AsyncMock, Mock, patch
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

from generalAgent.context.token_tracker import TokenTracker, ContextStatus
from generalAgent.context.manager import ContextManager
from generalAgent.context.compressor import CompressionResult
from generalAgent.config.settings import get_settings


@pytest.fixture
def settings():
    return get_settings()


@pytest.fixture
def tracker(settings):
    return TokenTracker(settings)


@pytest.fixture
def context_manager(settings):
    return ContextManager(settings)


@pytest.fixture
def sample_messages():
    """ç”Ÿæˆæµ‹è¯•æ¶ˆæ¯"""
    messages = [SystemMessage(content="System prompt")]
    for i in range(100):
        messages.append(HumanMessage(content=f"Question {i}" * 20))  # ~400 chars each
        messages.append(AIMessage(content=f"Answer {i}" * 20))
    return messages


class TestTokenTrackerCriticalDetection:
    """æµ‹è¯• TokenTracker çš„ critical çŠ¶æ€æ£€æµ‹"""

    def test_detect_critical_threshold(self, tracker):
        """æµ‹è¯•æ­£ç¡®æ£€æµ‹ critical é˜ˆå€¼ï¼ˆ>= 95%ï¼‰"""
        # 96% usage
        status = tracker.check_status(
            cumulative_prompt_tokens=123000,
            model_id="deepseek-chat"
        )

        assert status.level == "critical"
        assert status.needs_compression is True
        assert status.usage_ratio >= 0.95
        assert "ğŸš¨" in status.message

    def test_detect_warning_not_critical(self, tracker):
        """æµ‹è¯• warning çº§åˆ«ä¸è§¦å‘ criticalï¼ˆ85-95%ï¼‰"""
        # 90% usage
        status = tracker.check_status(
            cumulative_prompt_tokens=115000,
            model_id="deepseek-chat"
        )

        assert status.level == "warning"
        assert status.needs_compression is False

    def test_detect_info_not_critical(self, tracker):
        """æµ‹è¯• info çº§åˆ«ä¸è§¦å‘ criticalï¼ˆ75-85%ï¼‰"""
        # 80% usage
        status = tracker.check_status(
            cumulative_prompt_tokens=102000,
            model_id="deepseek-chat"
        )

        assert status.level == "info"
        assert status.needs_compression is False


class TestContextManagerCompression:
    """æµ‹è¯• ContextManager çš„å‹ç¼©é€»è¾‘"""

    @pytest.mark.asyncio
    async def test_compress_reduces_message_count(self, context_manager, sample_messages):
        """æµ‹è¯•å‹ç¼©æˆåŠŸå‡å°‘æ¶ˆæ¯æ•°é‡"""
        # Mock LLM invoker
        async def mock_invoker(prompt, max_tokens=2048):
            return "Compressed summary of conversation"

        result = await context_manager.compress_context(
            messages=sample_messages,
            model_invoker=mock_invoker,
            context_window=128000
        )

        # éªŒè¯å‹ç¼©æ•ˆæœ
        assert result.after_count < result.before_count
        assert len(result.messages) < len(sample_messages)
        assert result.compression_ratio < 1.0
        assert result.strategy == "compact"

    @pytest.mark.asyncio
    async def test_compress_resets_tokens(self, context_manager, sample_messages):
        """æµ‹è¯•å‹ç¼©å token ä¼°ç®—å‡å°‘"""
        async def mock_invoker(prompt, max_tokens=2048):
            return "Brief summary"

        result = await context_manager.compress_context(
            messages=sample_messages,
            model_invoker=mock_invoker,
            context_window=128000
        )

        # éªŒè¯ token å‡å°‘
        assert result.after_tokens < result.before_tokens
        saved_tokens = result.before_tokens - result.after_tokens
        assert saved_tokens > 0

    @pytest.mark.asyncio
    async def test_compress_preserves_system_messages(self, context_manager):
        """æµ‹è¯•å‹ç¼©ä¿ç•™ SystemMessage"""
        messages = [
            SystemMessage(content="System prompt 1"),
            SystemMessage(content="System prompt 2"),
            HumanMessage(content="User message"),
            AIMessage(content="AI response"),
        ]

        async def mock_invoker(prompt, max_tokens=2048):
            return "Summary"

        result = await context_manager.compress_context(
            messages=messages,
            model_invoker=mock_invoker,
            context_window=128000
        )

        # éªŒè¯ SystemMessage è¢«ä¿ç•™
        system_count = sum(1 for m in result.messages if isinstance(m, SystemMessage))
        assert system_count == 2


class TestAutoCompressionStateUpdate:
    """æµ‹è¯•è‡ªåŠ¨å‹ç¼©åçš„ state æ›´æ–°é€»è¾‘"""

    def test_state_update_after_compression(self):
        """æµ‹è¯•å‹ç¼©å state æ­£ç¡®æ›´æ–°"""
        # æ¨¡æ‹Ÿå‹ç¼©å‰çš„ state
        state = {
            "messages": [HumanMessage(content="msg")] * 100,
            "cumulative_prompt_tokens": 123000,
            "cumulative_completion_tokens": 5000,
            "compact_count": 0,
            "auto_compressed_this_request": False,
        }

        # æ¨¡æ‹Ÿå‹ç¼©ç»“æœ
        compressed_messages = [HumanMessage(content="msg")] * 50

        # åº”ç”¨æ›´æ–°é€»è¾‘
        state["messages"] = compressed_messages
        state["compact_count"] = state["compact_count"] + 1
        state["cumulative_prompt_tokens"] = 0
        state["cumulative_completion_tokens"] = 0
        state["auto_compressed_this_request"] = True

        # éªŒè¯
        assert len(state["messages"]) == 50
        assert state["compact_count"] == 1
        assert state["cumulative_prompt_tokens"] == 0
        assert state["cumulative_completion_tokens"] == 0
        assert state["auto_compressed_this_request"] is True

    def test_prevent_duplicate_compression_flag(self):
        """æµ‹è¯•é˜²é‡å¤å‹ç¼©æ ‡å¿—ç”Ÿæ•ˆ"""
        state = {"auto_compressed_this_request": False}

        # ç¬¬ä¸€æ¬¡æ£€æŸ¥ï¼šæœªå‹ç¼©
        if not state.get("auto_compressed_this_request", False):
            state["auto_compressed_this_request"] = True
            first_check_should_compress = True
        else:
            first_check_should_compress = False

        assert first_check_should_compress is True

        # ç¬¬äºŒæ¬¡æ£€æŸ¥ï¼šå·²å‹ç¼©
        if not state.get("auto_compressed_this_request", False):
            second_check_should_compress = True
        else:
            second_check_should_compress = False

        assert second_check_should_compress is False


class TestCompressionMaxTokensLimit:
    """æµ‹è¯•å‹ç¼©è¾“å‡ºçš„ max_tokens é™åˆ¶"""

    @pytest.mark.asyncio
    async def test_compression_uses_max_tokens_limit(self, context_manager, sample_messages):
        """æµ‹è¯•å‹ç¼©è°ƒç”¨ LLM æ—¶ä½¿ç”¨ max_tokens é™åˆ¶"""
        max_tokens_used = None

        async def mock_invoker(prompt, max_tokens=2048):
            nonlocal max_tokens_used
            max_tokens_used = max_tokens
            return "Summary"

        await context_manager.compress_context(
            messages=sample_messages,
            model_invoker=mock_invoker,
            context_window=128000
        )

        # éªŒè¯ä½¿ç”¨äº† max_tokens é™åˆ¶ï¼ˆ1440 tokens = 2000 chars + 20% bufferï¼‰
        assert max_tokens_used == 1440


class TestCompressionFallback:
    """æµ‹è¯•å‹ç¼©å¤±è´¥çš„é™çº§ç­–ç•¥"""

    @pytest.mark.asyncio
    async def test_fallback_to_truncation_on_error(self, context_manager, sample_messages):
        """æµ‹è¯•å‹ç¼©å¤±è´¥æ—¶é™çº§åˆ°ç®€å•æˆªæ–­"""
        # Mock invoker æŠ›å‡ºå¼‚å¸¸
        async def failing_invoker(prompt, max_tokens=2048):
            raise Exception("LLM compression failed")

        result = await context_manager.compress_context(
            messages=sample_messages,
            model_invoker=failing_invoker,
            context_window=128000
        )

        # éªŒè¯é™çº§ç­–ç•¥ç”Ÿæ•ˆ
        assert result.strategy == "emergency_truncate"
        assert len(result.messages) < len(sample_messages)
        # åº”è¯¥ä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯ (system messages + recent)
        assert len(result.messages) <= 101  # CONTEXT_MAX_HISTORY (100) + system messages


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
